{"cells":[{"cell_type":"markdown","metadata":{"id":"hLcWRu-jp7gM"},"source":["# MIS 583 Assignment 2: Binary Classification"]},{"cell_type":"markdown","metadata":{"id":"oSwr9MgZogRZ"},"source":["Before we start, please put your name and ID in following format  \n",": LASTNAME Firstname, ?000000000   //   e.g.) 李晨愷, M114020035\n","\n","**Your Answer:**   \n","Hi I'm XXX, XXXXXXXXXX"]},{"cell_type":"markdown","metadata":{"id":"xnqV2-Wd1AVu"},"source":["This assignment aims to work you through the core concepts in implementation a binary classifier. We'll start from implementing a logistic regression from scratch by our own (Part 1) and then implementing it using PyTorch APIs (Part 2)."]},{"cell_type":"markdown","metadata":{"id":"B_MoQiztpxcK"},"source":["## Logistic Regression\n","\n","Logistic regression predicts the probability that an event could occur and typically is used to perform binary classification.\n","\n","In the first two parts of this assignment, we will cover:\n","* PyTorch: Tensor operations\n","* Machine Learning: Data preprocessing, logistic Regression, gradient descent, \n","\n","This assignment will walk you through implementing a logistic regression model that classifies whether a person is rich or poor using the UCI adult income dataset in PyTorch.\n","\n","* In part 1, you will **implement a logistic regression from scratch using PyTorch tensors and tensor operations**. This will help you gain a better understanding of the theoretical concepts discussed in class.\n","* In part 2, you will **use PyTorch nn.Module to build a logsitic regression** so that you will get familiar with PyTorch APIs.\n","\n","Note that **You ARE NOT allowed to remove any assertions.**"]},{"cell_type":"markdown","metadata":{"id":"giUId1Naqacs"},"source":["##  Versions of used packages\n","\n","We will check PyTorch version to make sure everything work properly.\n","\n","I use `python 3.9.5`, `torch==1.8.2` and `torchvision==0.9.2`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vuw-gNvjqcYe"},"outputs":[],"source":["!python --version\n","import torch\n","import torchvision\n","print('torch', torch.__version__)\n","print('torchvision', torchvision.__version__)"]},{"cell_type":"markdown","metadata":{"id":"OhdbdJOsrbxL"},"source":["## Preparing Data"]},{"cell_type":"markdown","metadata":{"id":"gVbtJxl6rc3t"},"source":["### Loading Data"]},{"cell_type":"markdown","metadata":{"id":"nPoSgD83teTQ"},"source":["We use [adult income](https://www.kaggle.com/wenruliu/adult-income-dataset) dataset from UCI machine learning repository.  \n","\n","**Abstract**  \n","\n","Given an individual’s education level, age, gender, occupation, and etc, we want to predict his or her income level.   \n","\n","**Metadata**  \n","Number of attributes: 14  \n","- income: >50K, <=50K\n","- age: continuous.\n","- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n","- fnlwgt: continuous.\n","- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n","- education-num: continuous.\n","- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n","- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n","- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n","- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n","- sex: Female, Male.\n","- capital-gain: continuous.\n","- capital-loss: continuous.\n","- hours-per-week: continuous.\n","- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2113,"status":"ok","timestamp":1634222639645,"user":{"displayName":"陳耀融","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjj86gnUOXXpG-a6sSkG8YLW4QAl4FnFoA7sBkH=s64","userId":"02126975196668232417"},"user_tz":-480},"id":"FhKEp8b1r3V-","outputId":"e452291b-070d-4b75-b84b-19c0bc4aa7ce"},"outputs":[],"source":["# Download it from cu and upload to colab\n","\n","# or from the Dropbox\n","!wget -q -N https://www.dropbox.com/s/1jqeipgof7tukln/assign2_data.zip\n","!unzip -n assign2_data.zip"]},{"cell_type":"markdown","metadata":{"id":"cqO8DiB6VRQZ"},"source":["There are 4 csv files.\n","\n","\n","* X_train: training data \n","* X_test: training data labels\n","* Y_train: test data\n","* Y_test: test data labels.\n","\n","Each row in X has 106 fields.\n","Each row Y represents the true label of 0 (poor) and 1 (rich).  \n","\n","Or you can load train.csv raw data and do your pre-processing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRq8vX4kn24u"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Pre-Processed Version\n","X_train_raw = pd.read_csv('data/X_train.csv')\n","Y_train_raw = pd.read_csv('data/Y_train.csv')\n","X_test_raw = pd.read_csv('data/X_test.csv')\n","Y_test_raw = pd.read_csv('data/Y_test.csv')\n","\n","X_train_raw.head()\n","\n","# Or you can do preprocess by yourself\n","# df = pd.read_csv('data/train.csv')\n","# ...some preprocessing\n","# ...remember split into 4 variables with same name by yourself"]},{"cell_type":"markdown","metadata":{"id":"7XvstELNg7wO"},"source":["You may already notice that there are two datasets.   One is \"training set\", and the other is \"test set\".  \n","The training set is like **homework of model**. A model uses the training set to learn.  \n","The testing set is like **quiz of model**. A learned model is tested on the testing set for evaluating how good a model is.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ckRcWuGlZLYV"},"source":["### Pre-Processing Data (3 points)"]},{"cell_type":"markdown","metadata":{"id":"TGlPZ1Okx1Z0"},"source":["In a data science process, data cleansing usually cost the most of time.  \n","Dirty data will cause overfitting or make us ignore those important features. Even if you don't have correct cleansing and format, your model can't run anymore."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuuY4McyXrb3"},"outputs":[],"source":["import torch\n","################################################################################\n","# TODO: Implement Standard Deviation Normalization mentioned in class.         #\n","# train_data has shape(n_train, feature_dim)                                   #\n","# test_data has shape(n_test, feature_dim)                                     #\n","# hint: You should compute mean and std using training data, and apply it to   #\n","# test.                                                                        #\n","################################################################################\n","def compute_mu_std(data):\n","    '''\n","    Arguments:\n","    data -- torch.Tesor, contain data with shape(n, feature_dim)\n","\n","    Return:\n","    mu - torch.Tensor, mean value of columns of data with shape(feature_dim)\n","    sigma - torch.Tensor, std value of columns of data with shape(feature_dim)\n","    '''\n","    mu = ...\n","    sigma = ...\n","    return mu, sigma\n","\n","def standard_normalize(data, mu, std):\n","    '''\n","    Arguments:\n","    data -- torch.Tesor, contain data with shape(n, feature_dim)\n","    mu - torch.Tensor, mean value of columns of data with shape(feature_dim)\n","    sigma - torch.Tensor, std value of columns of data with shape(feature_dim)\n","\n","    Return:\n","    data -- torch.Tensor, normalized data with given mu and sigma\n","    '''\n","    data = ...\n","    return data\n","\n","# Better not to change code below, except you wanna do your own pre-processing\n","# pandas > (.values) > numpy > (tensor) > torch.tensor\n","X_train = torch.tensor(X_train_raw.values, dtype=torch.float)\n","X_test = torch.tensor(X_test_raw.values, dtype=torch.float)\n","\n","mu, sigma = compute_mu_std(X_train)\n","\n","# output the first 5 values of mu and sigma\n","print('the first 5 means are:', mu[:5])\n","print('the first 5 sigma are:', sigma[:5])\n","\n","f_dim = X_train.shape[1]\n","assert mu.shape == torch.Size([f_dim]), 'Shape of mu is incorrect.'\n","assert sigma.shape == torch.Size([f_dim]), 'Shape of sigma is incorrect.'\n","\n","X_train = standard_normalize(X_train, mu, sigma)\n","X_test = standard_normalize(X_test, mu, sigma)\n","################################################################################\n","#                             END OF YOUR CODE                                 #\n","################################################################################\n","# make shape(n, 1) > shape(n), like [[1, 2, 3]] > [1, 2, 3]\n","Y_train = torch.tensor(Y_train_raw.values).squeeze()\n","Y_test = torch.tensor(Y_test_raw.values).squeeze()\n","\n","print('Shape of X_train:', X_train.shape) \n","print('Shape of X_test:', X_test.shape)  \n","print('Shape of y_train:', Y_train.shape)\n","print('Shape of y_test:', Y_test.shape)\n","\n","\n","assert X_train.dim() == 2\n","assert Y_train.dim() == 1\n","# IF you are writting YOUR pre-processing, the tensors may have different shape.\n","assert X_train.shape == (32561, 106)\n","assert X_test.shape == (16281, 106)"]},{"cell_type":"markdown","metadata":{"id":"U5jtBkbJyBym"},"source":["Because the data used to train a deep model may be large, you could not load the model and the whole dataset into memory (GPU or RAM) at the same time.  \n","So, we split the data into mini-batches."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zX1pqlGnn25M"},"outputs":[],"source":["def make_batch(data, batch_size=128, drop_last=True):\n","    '''\n","    Split dataset into batches.\n","\n","    Arguments:\n","    data -- torch.Tensor, data with shape(n, ...)\n","    batch_size -- int, how many data in your batch\n","    drop_last -- boolean, drop last datas if your remaining data is < batch_size\n","\n","    Return:\n","    out -- torch.Tensor(dtype=torch.float) with shape(n_batch, batch_size, ...)\n","    '''\n","    if drop_last:\n","        n = data.shape[0] // batch_size\n","    else:\n","        n = ((data.shape[0] - 1) // batch_size) + 1\n","    out = np.empty(torch.Size((n, batch_size)) + data.shape[1:], dtype=np.float32)\n","    print(out.shape)\n","    for b in range(len(out)):\n","        out[b] = data[b * batch_size:(b + 1) * batch_size]\n","    return torch.tensor(out)\n","\n","X_train_batch = make_batch(X_train)\n","Y_train_batch = make_batch(Y_train)\n","X_test_batch = make_batch(X_test)\n","Y_test_batch = make_batch(Y_test)"]},{"cell_type":"markdown","metadata":{"id":"U4atwzT3aPi3"},"source":["We have prepared the data.  \n","Let's develop a logistic regression model."]},{"cell_type":"markdown","metadata":{"id":"87KYcWknS95z"},"source":["# Part 1: Implement Logistic Regression from Scratch"]},{"cell_type":"markdown","metadata":{"id":"Y_moLy20cEn_"},"source":["## Utility function"]},{"cell_type":"markdown","metadata":{"id":"OH_4NKB9dsZ3"},"source":["### Activation Function (3 points)"]},{"cell_type":"markdown","metadata":{"id":"ifQD-YnvcY3k"},"source":["Logistic Regression uses a logistic function (or called a sigmoid function) to turn a real value into a probability (a value between zero and one). Below depicts the sigmoid function.\n","\n","**Slide: ch02 p67**\n","\n","![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png)\n","\n","*Source: wikipedia*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CuzoweBRn25o"},"outputs":[],"source":["def sigmoid(z): \n","    '''\n","    Compute the sigmoid of z\n","\n","    Arguments:\n","    z -- A scalar or torch.Tensor of any size.\n","\n","    Return:\n","    s -- torch.Tensor, sigmoid(z)\n","    '''\n","    if not isinstance(z, torch.Tensor):\n","        z = torch.tensor(z, dtype=torch.float)\n","    ############################################################################\n","    # TODO: Implement sigmoid (or called logistic) function                     #\n","    # Slide: ch02 p67                                                         #\n","    ############################################################################\n","    pass\n","    ############################################################################\n","    #                             END OF YOUR CODE                             #\n","    ############################################################################\n","    # prevent out is Inf or -Inf\n","    out = torch.clamp(out, 1e-6, 1-1e-6)\n","    return out\n","\n","s_t1 = sigmoid(0)\n","s_t2 = sigmoid(2)\n","s_t3 = sigmoid(-1)\n","print('sigmoid(0)  =', s_t1)\n","print('sigmoid(2)  =', s_t2)\n","print('sigmoid(-1) =', s_t3)\n","msg = 'Your sigmoid: {} isn\\'t correct'\n","assert (s_t1 - 0.5) < 0.001, msg.format(sigmoid(0))\n","assert (s_t2 - 0.8808) < 0.001, msg.format(sigmoid(2))\n","assert (s_t3 - 0.2689) < 0.001, msg.format(sigmoid(-1))"]},{"cell_type":"markdown","metadata":{"id":"LVnUmO0DdwjF"},"source":["### Loss Function (3 points)"]},{"cell_type":"markdown","metadata":{"id":"inIo1GtRdylu"},"source":["Many machine learning applications use the cross-entropy loss as described in class. This loss can be directly derived from information theoretic considerations, which measures the differences between two probability distributions.\n","In this assignment, we will use the binary cross entropy loss for binary classification.\n","**Slide: ch02 p74**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sboq4fXhn250"},"outputs":[],"source":["def binary_cross_entropy(y_true, y_pred):\n","    '''\n","    Compute the binary cross entropy of inputs.\n","\n","    Arguments:\n","    y_true -- torch.Tensor, True data with shape(n_size).\n","    y_pred -- torch.Tensor, Predicted data with shape(n_size)\n","\n","    Return:\n","    s -- torch.Tensor, binary_cross_entropy(z)\n","    '''\n","    ############################################################################\n","    # TODO: Implement binary cross entropy                                     #\n","    # Slide: ch02 p74                                                        #\n","    ############################################################################\n","    loss = ...\n","    ############################################################################\n","    #                             END OF YOUR CODE                             #\n","    ############################################################################\n","    return loss\n","\n","b_t1 = binary_cross_entropy(torch.tensor([1]), torch.tensor([0.5]))\n","b_t2 = binary_cross_entropy(torch.tensor([1]), torch.tensor([0.8]))\n","b_t3 = binary_cross_entropy(torch.tensor([1]), torch.tensor([0.9]))\n","print('bce(1, 0.5)', b_t1)\n","print('bce(1, 0.8)', b_t2)\n","print('bce(1, 0.9)', b_t3)\n","msg = 'Your bce: {} isn\\'t correct'\n","assert (b_t1 - 0.6931) < 0.001, msg.format(b_t1)\n","assert (b_t2 - 0.2231) < 0.001, msg.format(b_t1)\n","assert (b_t3 - 0.1054) < 0.001, msg.format(b_t1)"]},{"cell_type":"markdown","metadata":{"id":"0OswC-h-e1-0"},"source":["## Logistic Regression Model (4 points)"]},{"cell_type":"markdown","metadata":{"id":"uu-Ls8DefPGK"},"source":["Let's implement the training algorithm!  \n","\n","**Slide: ch02 p106 Training algorithm with mini-batch SGD**\n"," \n","```python\n","num_epochs = ... # specify the number of epochs to train initialize parameters w, b\n","for epoch in range(num_epochs):\n","    shuffle training data\n","    for each batch:\n","        forward propagation to get the predictions/outputs \n","        compute loss\n","        backward propagation to get the gradients\n","        update parameters using their gradients\n","```"]},{"cell_type":"markdown","metadata":{"id":"cglTIH1NrdIO"},"source":["You should fill out each methods (init, forward, backward, optimizer...)  \n","Inputs, outputs, format and description is written in docstring `'''doc string there'''`  \n","**You are NOT allowed to remove any assertion.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LOzsSk7n26G"},"outputs":[],"source":["class LogisticRegression:\n","    def __init__(self, lr, feature_dim):\n","        '''\n","        Initialize the learning rate\n","        Initialize the weights and bias to zeros with correct shapes.\n","        Note that w and b should be torch.tensor(...)\n","        The shape of w should be (feature_dim)\n","        The shape of b should be (1)\n","\n","        Arguments:\n","        lr -- float, learning rate (0 < lr <= 1) to control the step of updates.\n","        feature_dim -- int, the number of features.\n","        '''\n","        ########################################################################\n","        #                           Your code there                            #\n","        ########################################################################\n","        self.w = ...\n","        self.b = ...\n","        self.lr = ...\n","        ########################################################################\n","        #                           End of your code                           #\n","        ########################################################################\n","        assert self.w.shape == torch.Size([feature_dim]), 'shape of w is incorrect'\n","        assert self.b.dtype == torch.float, 'dtype of b should be float'\n","\n","        # initialize dw, db to zeros\n","        self.grads = {'dw': torch.zeros(feature_dim, dtype=torch.float), \n","                      'db': torch.tensor(0, dtype=torch.float)}\n","\n","    def forward(self, x):\n","        '''\n","        Compute the predicted probability using the w and b for an given input x.\n","        Store the prediction in self.out, which will be needed in backward pass.\n","\n","        Arguments: \n","        x -- torch.Tensor, input data with shape(n, feature_dim).\n","\n","        Return:\n","        out -- sigmoid(x * w + b)\n","        '''\n","        ########################################################################\n","        #                           Your code there                            #\n","        ########################################################################\n","        out = ...\n","        out = ...\n","        ########################################################################\n","        #                           End of your code                           #\n","        ########################################################################\n","        self.out = out\n","        assert (out > 0).all() and (out < 1).all(), 'Output should > 0 and < 1'\n","        assert out.shape == torch.Size([x.shape[0]]), 'Shape of output is incorrect.'\n","        return self.out\n","    \n","    def backward(self, x, y_true):\n","        '''\n","        After the forward pass, calculate gradients of w and b (dw and db).\n","        Store dw and db in self.grads.\n","        Ref: Slide ch02 102\n","\n","        Arguments:\n","        x -- torch.Tensor, input data with shape(n,).\n","        y_true -- torch.Tensor, true data with shape(n,).\n","        '''\n","        y_pred = self.out\n","        ########################################################################\n","        #                           Your code there                            #\n","        # Slide: ch02 p102 Gradient descent implementation                      #\n","        ########################################################################\n","        dw = ...\n","        db = ...\n","        ########################################################################\n","        #                           End of your code                           #\n","        ########################################################################\n","\n","        assert dw.shape == self.w.shape, 'Shape of dw is incorrect.'\n","        assert db.shape == self.b.shape, 'Shape of db is incorrect.'\n","\n","        self.grads = {'dw': dw, 'db': db}\n","\n","    def optimize(self):\n","        '''\n","        Implement the mini-batch SGD algorithm.\n","        Use gradients and lr to update the weights and bias.\n","        '''\n","        ########################################################################\n","        #                           Your code there                            #\n","        ########################################################################\n","        pass\n","        ########################################################################\n","        #                           End of your code                           #\n","        ########################################################################\n","\n","    def predict(self, x):\n","        '''\n","        Compute the probability prediction of you model and threshold the \n","        probability output at 0.5 to obtain the label.\n","        hint: you should re-use forward method\n","\n","        Arguments:\n","        x -- torch.Tensor, input data with shape(n, feature_dim).\n","\n","        Return:\n","        out -- sigmoid(x * w + b) > 0.5 with shape(n,)\n","        '''\n","        ########################################################################\n","        #                           Your code there                            #\n","        ########################################################################\n","        out = ...\n","        ########################################################################\n","        #                           End of your code                           #\n","        ########################################################################\n","        assert out.shape == torch.Size([x.shape[0]]), 'Shape of output is incorrect'\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RMn82737BAW"},"outputs":[],"source":["################################################################################\n","#                             Hyperparameters                                 #\n","# You can modify these hyperparameters to see how they affect the accuracy    #\n","################################################################################\n","lr = 3e-2\n","max_epochs = 50\n","log_interval = 5\n","\n","model = LogisticRegression(lr=lr, feature_dim=X_train.shape[1])\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_loss_list = []\n","test_acc_list = []"]},{"cell_type":"markdown","metadata":{"id":"KUzf_a3-aa6F"},"source":["### Putting everything together (3 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXzuzR8Xn26N"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","for epoch in range(1, max_epochs + 1):\n","    idxs = torch.randperm(X_train_batch.shape[0]) # make rand idx\n","    data_loader = ((X_train_batch[i], Y_train_batch[i]) for i in idxs)\n","\n","    loss_list = [] # save train_loss\n","    for i, (x, y) in enumerate(data_loader):\n","        ########################################################################\n","        # TODO: Combine all the functions together                             #\n","        ########################################################################\n","        # hints:\n","        # 1. get output from forward method, call forward()\n","        # 2. calculate loss using binary_cross_entropy()\n","        # 3. using backward method to calculate dw, db, call backward()\n","        # 4. apply SGD optimize to update model's weights, call optimize()\n","        pass\n","        ########################################################################\n","        #                         END OF YOUR CODE                             #\n","        ########################################################################\n","\n","        loss_list.append(loss)\n","\n","    train_loss_list.append(sum(loss_list) / len(loss_list))\n","    train_acc_list.append(accuracy_score(Y_train, model.predict(X_train.float())))\n","    test_loss_list.append(binary_cross_entropy(Y_test, model.forward(X_test.float())))\n","    test_acc_list.append(accuracy_score(Y_test, model.predict(X_test.float())))\n","    if epoch % log_interval == 0:\n","        print('=' * 20, 'Epoch', epoch, '=' * 20)\n","        print('Train loss:', train_loss_list[-1], 'acc:', train_acc_list[-1])\n","        print('Test loss: ', test_loss_list[-1], 'acc:', test_acc_list[-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrC0_PiyMgfW"},"outputs":[],"source":["# plot loss and acc\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 4))\n","plt.plot(range(len(train_loss_list)), train_loss_list)\n","plt.plot(range(len(test_loss_list)), test_loss_list, c='r')\n","plt.legend(['train', 'test'])\n","plt.title('Loss')\n","plt.show()\n","plt.figure(figsize=(12, 4))\n","plt.plot(range(len(train_acc_list)), train_acc_list)\n","plt.plot(range(len(test_acc_list)), test_acc_list, c='r')\n","plt.legend(['train', 'test'])\n","plt.title('Acc')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Hnuy34aKoc2f"},"source":["Congratulations! You've made it."]},{"cell_type":"markdown","metadata":{"id":"kgIWwcZIn263"},"source":["# Implement Logsitic Regression Using PyTorch nn.Module APIs"]},{"cell_type":"markdown","metadata":{"id":"-2fKjMvVkPWo"},"source":["Now, let's learn to use PyTorch APIs with a few lines to implement a logistic regression model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nuzoiHyan264"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"s6SNWep2zNmF"},"source":["Let's start with the PyTorch Dataset.  \n","You may notice that there are the dataset and the data_loader classes.\n","The Dataset handles the whole dataset.\n","The Dataloader provides batching, shuffling and many more utility functions about Dataset.  \n","We will dicuss more about it later.\n","\n","Use `TensorDataset` and `DataLoader` from `torch.utils.data`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vlv-ziS2g94b"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader\n","\n","train_dataset = TensorDataset(X_train, Y_train.unsqueeze(dim=1).float())\n","test_dataset = TensorDataset(X_test, Y_test.unsqueeze(dim=1).float())\n","train_data = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","test_data = DataLoader(test_dataset, batch_size=128, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"5COtAwHQ0p3a"},"source":["PyTorch provides very convenient ways to construct a model. \n","1. All models should inherit `nn.Module`.  \n","2. In `__init__` method, remember to call `super().__init__()` and define the network layers.  \n","3. In `forward()` method, use the defined layers to build a network model.\n","\n","PyTorch will use [autograd mechanic](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) to generate `backward()` and get gradients **automatically**. So, we don't need to worry about the gradient computation."]},{"cell_type":"markdown","metadata":{"id":"BnGYWc25blAF"},"source":["### Define a logistic regression using nn.Module (3 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sk5puwUJn27F"},"outputs":[],"source":["class TorchLogisticRegression(nn.Module):\n","    def __init__(self, feature_dim):\n","        '''\n","        Initiate weights and bias of our model.\n","\n","        Arguments:\n","        feature_dim -- int, How many features your data have.\n","        '''\n","        super().__init__()\n","        # init the weight AND bias by nn.Linear\n","        ########################################################################\n","        # TODO: use nn.xxx method to generate a linear model part              #\n","        #   you can define one layer with wieght and bias                      #\n","        ########################################################################\n","        self.linear = ...\n","        ########################################################################\n","        #                           End of your code                           #\n","        ########################################################################\n","    \n","    def forward(self, x):\n","        if not isinstance(x, torch.Tensor):\n","            x = torch.Tensor(x)\n","        ########################################################################\n","        # TODO: forward your model and get output                              #\n","        #   Don't forget activation function. But, you can't use previous      #\n","        #   defined sigmoid, try to search in PyTorch docs                     #\n","        ########################################################################\n","        out = ...\n","        assert out.shape == torch.Size([x.shape[0], 1]), 'Shape of output is incorrect'\n","        ########################################################################\n","        #                           End of your code                           #\n","        ########################################################################\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"83imqLRQ0v7M"},"source":["Once the model is defined, we can proceed to define the loss (e.g., binary cross entropy) and optimizer (e.g., SGD).\n","\n","* [Pedefined losses](https://pytorch.org/docs/stable/nn.html)\n","* [Optimizers](https://pytorch.org/docs/stable/optim.html)\n","\n","Also, learn the difference between nn.BCELoss and nn.BCEWithLogitsLoss."]},{"cell_type":"markdown","metadata":{"id":"FtOkPO6Ga0Fw"},"source":["### Define loss and optimizer (3 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yIRtfYan27M"},"outputs":[],"source":["# 106 features in our income dataset\n","torch_model = TorchLogisticRegression(X_train.shape[1])\n"," \n","########################################################################\n","# TODO: Define loss and optmizer functions                             #\n","#   please use Binary Cross Entropy and SGD optimizer                  #\n","# hint: torch.nn and torch.optim                                       #\n","########################################################################\n","criterion = ... # define the loss\n","params = ... # call a method of model to get parameters\n","optimizer = ... # throw param into optimizer some_optimier(param, lr=...)\n","########################################################################\n","#                           End of your code                           #\n","########################################################################"]},{"cell_type":"markdown","metadata":{"id":"zle9KuFcbwMP"},"source":["### Train the model (3 points)"]},{"cell_type":"markdown","metadata":{"id":"bZFxE7Y9iLfl"},"source":["Let's define train function.  \n","It will iterate the inputed data 1 epoch and update model with optmizer.  \n","Finally, calculate mean loss and total accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VM93brDshO6E"},"outputs":[],"source":["def train(data, model, criterion, optimizer):\n","    '''\n","    Argement:\n","    data -- iterable data, typr torch.utils.data.Dataloader is prefer\n","    model -- nn.Module, model contain forward to predict output\n","    criterion -- loss function, used to evaluate goodness of model\n","    optimizer -- optmizer function, method for weight updating\n","    '''\n","    model.train()\n","    \n","    loss_list = []\n","    total_count = 0\n","    acc_count = 0\n","    for x, y in data:\n","        optimizer.zero_grad()\n","\n","        ########################################################################\n","        # Training part with data, model, criterion and optimizer              #\n","        ########################################################################\n","        # Like the training part we implemented above, but a PyTorch version\n","        # 1. get the output from the model, use model()\n","        # 2. calculate the loss using criterion(y_pred, y_true)\n","        # 3. backward method of loss to calculate gradient\n","        # 4. call the optimizer to update model's weights\n","        ########################################################################\n","        #                           End of your code                           #\n","        ########################################################################\n","\n","        total_count += out.shape[0]\n","        acc_count += ((out > 0.5) == y).sum().item()\n","        loss_list.append(loss.item())\n","    acc = acc_count / total_count\n","    loss = sum(loss_list) / len(loss_list)\n","    return acc, loss"]},{"cell_type":"markdown","metadata":{"id":"KmDy1GTq_H2a"},"source":["Next, we write the test function that works similarly as the training function but without the optmizer and weigght-updating parts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"USzbBgGEoTRu"},"outputs":[],"source":["def test(data, model, criterion):\n","    model.eval()\n","    \n","    loss_list = []\n","    total_count = 0\n","    acc_count = 0\n","    with torch.no_grad():\n","        for x, y in data:\n","            ####################################################################\n","            # Testing part with data, model and criterion                      #\n","            ####################################################################\n","            # Like training part without weight updating\n","            # 1. get output from model, use model()\n","            # 2. calculate loss using criterion(y_pred, y_true)\n","            ####################################################################\n","            #                           End of your code                       #\n","            ####################################################################\n","\n","            total_count += out.shape[0]\n","            acc_count += ((out > 0.5) == y).sum().item()\n","            loss_list.append(loss.item())\n","\n","    acc = acc_count / total_count\n","    loss = sum(loss_list) / len(loss_list)\n","    return acc, loss"]},{"cell_type":"markdown","metadata":{"id":"knXu74jCiuxP"},"source":["Finally, call the train and test functions in a loop. We also record the training/test losses and accuracies during the course of training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcVulKkFJRtI"},"outputs":[],"source":["# Hyper Parameters\n","max_epochs = 50\n","log_interval = 5\n","\n","train_acc_list = []\n","train_loss_list = []\n","test_acc_list = []\n","test_loss_list = []\n","\n","for epoch in range(1, max_epochs + 1):\n","    train_acc, train_loss = train(train_data, torch_model, criterion, optimizer)\n","    test_acc, test_loss = test(test_data, torch_model, criterion)\n","\n","    train_acc_list.append(train_acc)\n","    train_loss_list.append(train_loss)\n","    test_acc_list.append(test_acc)\n","    test_loss_list.append(test_loss)\n","    if epoch % log_interval == 0:\n","        print('=' * 20, 'Epoch', epoch, '=' * 20)\n","        print('Train Acc: {:.6f} Train Loss: {:.6f}'.format(train_acc, train_loss))\n","        print('Test Acc: {:.6f} Test Loss: {:.6f}'.format(test_acc, test_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ifzgfp7iq2m"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 4))\n","plt.plot(range(len(train_loss_list)), train_loss_list)\n","plt.plot(range(len(test_loss_list)), test_loss_list, c='r')\n","plt.legend(['train', 'test'])\n","plt.title('Loss')\n","plt.show()\n","plt.figure(figsize=(12, 4))\n","plt.plot(range(len(train_acc_list)), train_acc_list)\n","plt.plot(range(len(test_acc_list)), test_acc_list, c='r')\n","plt.legend(['train', 'test'])\n","plt.title('Acc')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wX87wzXhmYg"},"outputs":[],"source":["# Comparison of our model and torch's model\n","print('-' * 20, 'weight' , '-' * 20)\n","print('Ours', model.w[:20])\n","print('PyTorch', torch_model.linear.weight[0][:20])\n","print('-' * 20, 'bias' , '-' * 20)\n","print('Ours', model.b)\n","print('PyTorch', torch_model.linear.bias)"]},{"cell_type":"markdown","metadata":{"id":"4XTB6EF6mH2Q"},"source":["Our own implementation and the implementation using the PyTorch APIs obtain similar weights and biases.  \n","But less effort is needed when using PyTorch APIs!  "]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"},"vscode":{"interpreter":{"hash":"aaa478f9632825e83f6a2247407c7a2930de96a6810af7910643e423346524f9"}}},"nbformat":4,"nbformat_minor":0}
