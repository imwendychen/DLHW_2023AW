{"cells":[{"cell_type":"markdown","metadata":{"id":"30_IaEbtUOIp"},"source":["# MIS 583 A7 Semantic Segmentation on BCSS"]},{"cell_type":"markdown","metadata":{"id":"qZU2QAc7UOIr"},"source":["Before we start, please put your name and SID in following format: <br>\n",": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷 M114020035"]},{"cell_type":"markdown","metadata":{"id":"Iw16xFg1UOIs"},"source":["**Your Answer:**    \n","Hi I'm XXX, XXXXXXXXXX."]},{"cell_type":"markdown","metadata":{"id":"TjiGBabcUOIs"},"source":["## Overview\n","\n","- Semantic segmentation is a computer vision task that aims to classify each pixel in an image into specific objects or regions.\n","\n","- In this assignment, you will implement a segmentor to classify the specific types of breast cancer lesions.\n","\n","- The segmentor implemented in this assignment is U-Net, and you are required to construct it from scratch\n"]},{"cell_type":"markdown","metadata":{"id":"BAH6BJAqUOIs"},"source":["## Kaggle Competition\n","Kaggle is an online community of data scientists and machine learning practitioners. Kaggle allows users to find and publish datasets, explore and build models in a web-based data-science environment, work with other data scientists and machine learning engineers, and enter competitions to solve data science challenges.\n","\n","This assignment use kaggle to calculate your grade.  \n","Please use this [**LINK**](https://www.kaggle.com/t/13040d09b42940ba90f96cca324ee90d) to join the competition."]},{"cell_type":"markdown","metadata":{"id":"QBqTmnxpUOIt"},"source":["## Google Colab Setup\n","Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BYH-qNFxUOIt","executionInfo":{"status":"ok","timestamp":1703580690260,"user_tz":-480,"elapsed":22966,"user":{"displayName":"陳文薇","userId":"06611373830672607992"}},"outputId":"d0639b69-fd2e-4b5a-849a-ecb16682f8b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"626DMCr1UOIu"},"source":["## How to Get Data\n","\n","請先到共用雲端硬碟將檔案 `BCSS.zip`，建立捷徑到自己的雲端硬碟中。\n","\n","> 操作步驟\n","1. 點開雲端[連結](https://drive.google.com/file/d/1uW9SInf6jrF5YUCjf-2ObxLolYYEb2Np/view?usp=sharing)\n","2. 點選右上角「新增雲端硬碟捷徑」\n","3. 點選「我的雲端硬碟」\n","4. 點選「新增捷徑」\n","\n","完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"]},{"cell_type":"markdown","metadata":{"id":"ddPpRtTjUOIu"},"source":["## Unzip Data\n","\n","解壓縮 `BCSS.zip`\n","\n","+ `train` : 包含了train的所有圖片\n","+ `val` : 包含了val的所有圖片\n","+ `test` : 包含了test的所有圖片\n","+ `train_mask` : 包含了train的所有mask\n","+ `val_mask` : 包含了val所有mask\n","+ `test_mask` : 包含了test的所有mask\n","\n","\n","其中`train`的圖片 30760 張，`val`的圖片 5429 張，`test` 的圖片 4021 張。\n","\n","注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"b2HMQFljUOIv","executionInfo":{"status":"ok","timestamp":1703580776857,"user_tz":-480,"elapsed":80356,"user":{"displayName":"陳文薇","userId":"06611373830672607992"}}},"outputs":[],"source":["!unzip -qq ./drive/MyDrive/BCSS.zip"]},{"cell_type":"markdown","metadata":{"id":"l5i3N-weKP6-","papermill":{"duration":0.022866,"end_time":"2021-02-18T10:06:03.865074","exception":false,"start_time":"2021-02-18T10:06:03.842208","status":"completed"},"tags":[]},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T09:51:23.323783Z","iopub.status.busy":"2023-10-07T09:51:23.32341Z","iopub.status.idle":"2023-10-07T09:51:46.33636Z","shell.execute_reply":"2023-10-07T09:51:46.335174Z","shell.execute_reply.started":"2023-10-07T09:51:23.323704Z"},"id":"kFSYoYRqJSfB","papermill":{"duration":23.025453,"end_time":"2021-02-18T10:06:26.911588","exception":false,"start_time":"2021-02-18T10:06:03.886135","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms as T\n","import torchvision\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from PIL import Image\n","import cv2\n","import albumentations as A\n","\n","import time\n","import os\n","from tqdm.notebook import tqdm\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"1_HWVOdhJSfP","papermill":{"duration":0.026783,"end_time":"2021-02-18T10:06:26.966143","exception":false,"start_time":"2021-02-18T10:06:26.93936","status":"completed"},"tags":[]},"source":["# Preprocessing\n","\n","#### Verify that the paths to the image and mask are correct, and plot the position of the mask on the image.\n","\n","First, the code establishes DataFrames for the paths of training and validation images, including their corresponding mask paths.\n","\n","Second, it presents the total number of images in both the training and validation sets.\n","\n","Third, the code loads a sample training image and its associated mask, visually demonstrating the image and the applied mask in a graphical representation."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T09:51:46.338892Z","iopub.status.busy":"2023-10-07T09:51:46.338474Z","iopub.status.idle":"2023-10-07T09:51:46.342763Z","shell.execute_reply":"2023-10-07T09:51:46.341799Z","shell.execute_reply.started":"2023-10-07T09:51:46.338854Z"},"papermill":{"duration":0.033799,"end_time":"2021-02-18T10:06:27.027862","exception":false,"start_time":"2021-02-18T10:06:26.994063","status":"completed"},"tags":[],"trusted":true,"id":"Dr1-uZ4_UOIv"},"outputs":[],"source":["TRAIN_IMAGE_PATH = '/BCSS/train/'\n","VAL_IMAGE_PATH = '/BCSS/val/'\n","TRAIN_MASK_PATH = '/BCSS/train_mask/'\n","VAL_MASK_PATH = '/BCSS/val_mask/'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T09:51:46.344808Z","iopub.status.busy":"2023-10-07T09:51:46.344281Z","iopub.status.idle":"2023-10-07T09:51:46.56445Z","shell.execute_reply":"2023-10-07T09:51:46.563454Z","shell.execute_reply.started":"2023-10-07T09:51:46.344771Z"},"id":"gmPIy2ZFJSfQ","papermill":{"duration":0.157328,"end_time":"2021-02-18T10:06:27.211938","exception":false,"start_time":"2021-02-18T10:06:27.05461","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["n_classes = 3\n","\n","def create_df(IMAGE_PATH):\n","    name = []\n","    for dirname, _, filenames in os.walk(IMAGE_PATH):\n","        for filename in filenames:\n","            name.append(filename.split('.')[0])\n","    name.sort()\n","    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n","\n","train_df = create_df(TRAIN_IMAGE_PATH)\n","val_df = create_df(VAL_IMAGE_PATH)\n","\n","\n","print('Total Train Images: ', len(train_df))\n","print('Total Val Images: ', len(val_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYTnzYT8UOIw"},"outputs":[],"source":["X_train = train_df['id'].to_numpy()\n","X_val = val_df['id'].to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T09:51:46.58284Z","iopub.status.busy":"2023-10-07T09:51:46.58261Z","iopub.status.idle":"2023-10-07T09:51:50.573511Z","shell.execute_reply":"2023-10-07T09:51:50.572598Z","shell.execute_reply.started":"2023-10-07T09:51:46.582818Z"},"id":"-uXF6vetJSff","papermill":{"duration":4.280771,"end_time":"2021-02-18T10:06:31.595115","exception":false,"start_time":"2021-02-18T10:06:27.314344","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["img = Image.open(TRAIN_IMAGE_PATH + train_df['id'][100] + '.png')\n","mask = Image.open(TRAIN_MASK_PATH + train_df['id'][100] + '.png')\n","print('Image Size', np.asarray(img).shape)\n","print('Mask Size', np.asarray(mask).shape)\n","\n","\n","plt.imshow(img)\n","plt.imshow(mask, alpha=0.6)\n","plt.title('Picture with Mask Appplied')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dkjFEoe7JSfo","papermill":{"duration":0.030107,"end_time":"2021-02-18T10:06:31.656378","exception":false,"start_time":"2021-02-18T10:06:31.626271","status":"completed"},"tags":[]},"source":["# Loading the Dataset\n","\n","Define the BCSSDataset for loading the dataset, where each sample comprises an image and its corresponding mask\n","\n","Build a classs inherit `torch.utils.data.Dataset`.\n","  \n","Implement `__init__`, `__getitem__` and `__len__` 3 functions.  \n","\n","Some operations could be there: setting location of dataset, the method of reading data, label of dataset or transform of dataset.\n","\n","See [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) for more details"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T09:51:50.576239Z","iopub.status.busy":"2023-10-07T09:51:50.575909Z","iopub.status.idle":"2023-10-07T09:51:50.729274Z","shell.execute_reply":"2023-10-07T09:51:50.728347Z","shell.execute_reply.started":"2023-10-07T09:51:50.57618Z"},"id":"yne-vteuJSfp","papermill":{"duration":0.045449,"end_time":"2021-02-18T10:06:31.732603","exception":false,"start_time":"2021-02-18T10:06:31.687154","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class BCSSDataset(Dataset):\n","\n","    def __init__(self, img_path, mask_path, X, mean, std, transform=None):\n","        self.img_path = img_path\n","        self.mask_path = mask_path\n","        self.X = X\n","        self.transform = transform\n","        self.mean = mean\n","        self.std = std\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        img = cv2.imread(self.img_path + self.X[idx] + '.png')\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n","\n","        if self.transform is not None:\n","            aug = self.transform(image=img, mask=mask)\n","            img = Image.fromarray(aug['image'])\n","            mask = aug['mask']\n","\n","        if self.transform is None:\n","            img = Image.fromarray(img)\n","\n","        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n","        img = t(img)\n","        mask = torch.from_numpy(mask).long()\n","\n","        return img, mask"]},{"cell_type":"markdown","metadata":{"id":"xUaOpUZIUOIw"},"source":["## Data augmentation\n","\n","Data augmentation are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n","\n","PyTorch use `torchvision.transforms` to do data augmentation.\n","[You can see all function here.](https://pytorch.org/vision/stable/transforms.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T09:51:50.731573Z","iopub.status.busy":"2023-10-07T09:51:50.730886Z","iopub.status.idle":"2023-10-07T09:51:50.746718Z","shell.execute_reply":"2023-10-07T09:51:50.745914Z","shell.execute_reply.started":"2023-10-07T09:51:50.731536Z"},"id":"CCL93_giJSft","papermill":{"duration":0.041706,"end_time":"2021-02-18T10:06:31.80439","exception":false,"start_time":"2021-02-18T10:06:31.762684","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["mean=[0.485, 0.456, 0.406]\n","std=[0.229, 0.224, 0.225]\n","\n","# For TRAIN\n","########################################################################\n","#  TODO: use transforms.xxx method to do some data augmentation        #\n","#  This one is for training, find the composition to get better result #\n","########################################################################\n","transforms_train = ...\n","########################################################################\n","#                           End of your code                           #\n","########################################################################\n","\n","# For VAL\n","########################################################################\n","#  TODO: use transforms.xxx method to do some data augmentation        #\n","#  This one is for validate and test,                                  #\n","#  NOTICE some operation we usually not use in this part               #\n","########################################################################\n","transforms_val = ...\n","########################################################################\n","#                           End of your code                           #\n","########################################################################\n","\n","#datasets\n","train_set = BCSSDataset(TRAIN_IMAGE_PATH, TRAIN_MASK_PATH, X_train, mean, std, transforms_train)\n","val_set = BCSSDataset(VAL_IMAGE_PATH, VAL_MASK_PATH, X_val, mean, std, transforms_val)\n","\n","#dataloader\n","batch_size= 8\n","\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQgF1-hUUOIx"},"outputs":[],"source":["train_set[0]"]},{"cell_type":"markdown","metadata":{"id":"jJ-ZypiFJSgJ","papermill":{"duration":0.029797,"end_time":"2021-02-18T10:06:31.864199","exception":false,"start_time":"2021-02-18T10:06:31.834402","status":"completed"},"tags":[]},"source":["# U-net\n","\n","U-net is a fully convolution neural network for image semantic segmentation. Consist of encoder and decoder parts connected with skip connections. Encoder extract features of different spatial resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use concatenation for fusing decoder blocks with skip connections.\n","\n","\n","![image](https://hackmd.io/_uploads/rJXtsY_Up.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6c5s4oAUOIx"},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    \"\"\"\n","    Block with two convolutional blocks\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        \"\"\"\n","        Double convolution\n","\n","        :param in_channels: number of in channels for first conv layer\n","        :param out_channels: number of out channels for last conv layer\n","        :param mid_channels: number of out channels for first conv layer\n","        \"\"\"\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        ########################################################################\n","        #  TODO:                                                               #\n","        #  write model that contains 2 conv layer with batch normalization     #\n","        #  and relu activation function                                        #\n","        ########################################################################\n","\n","        self.double_conv = nn.Sequential(\n","            ...\n","        )\n","        ########################################################################\n","        #                           End of your code                           #\n","        ########################################################################\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    \"\"\"\n","    Block for down path\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"\n","        Down block\n","\n","        :param in_channels: number of in channels for double conv block\n","        :param out_channels: number of out channels for double conv block\n","        \"\"\"\n","        super().__init__()\n","        ########################################################################\n","        #  TODO:                                                               #\n","        #  write model which contains pooling and double conv block            #\n","        ########################################################################\n","        self.maxpool_conv = nn.Sequential(\n","            ...\n","        )\n","        ########################################################################\n","        #                           End of your code                           #\n","        ########################################################################\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","\n","class Up(nn.Module):\n","    \"\"\"\n","    Block for up path\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"\n","        Down block\n","\n","        :param in_channels: number of in channels for transpose convolution\n","        :param out_channels: number of out channels for double conv block\n","        \"\"\"\n","        super().__init__()\n","\n","        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","\n","        #########################################################################################\n","        # 1. Up-sample the input tensor using transpose convolution                             #\n","        # 2. Calculate the difference in height and width between x2 and the up-sampled x1      #\n","        # 3. Pad the up-sampled x1 to match the size of x2                                      #\n","        # 4. Concatenate x2 and the up-sampled x1 along the channel dimension                   #\n","        #########################################################################################\n","        x1 = ...\n","\n","        diffY = ...\n","        diffX = ...\n","\n","        x1 = ...\n","\n","        x = ...\n","\n","        return self.conv(x)\n","\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"\n","        Final convolution block\n","\n","        :param in_channels: number of in channels for conv layer\n","        :param out_channels: number of out channels for conv layer\n","        \"\"\"\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OJuKi2bUOIx"},"outputs":[],"source":["class UNet(nn.Module):\n","    \"\"\"\n","    UNet model\n","    \"\"\"\n","\n","    def __init__(self, n_channels, n_classes):\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","\n","        #############################################################\n","        #  implement the model according to the UNet architecture   #\n","        #############################################################\n","        self.inc = DoubleConv(n_channels, 64)\n","        self.down1 = ...\n","        self.down2 = ...\n","        self.down3 = ...\n","        self.down4 = ...\n","        self.up1 = ...\n","        self.up2 = ...\n","        self.up3 = ...\n","        self.up4 = ...\n","        self.outc = ...\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = ...\n","        x3 = ...\n","        x4 = ...\n","        x5 = ...\n","        x = ...\n","        x = ...\n","        x = ...\n","        x = ...\n","        logits = ...\n","        return logits\n","        #############################################################\n","        #                   End of your code                        #\n","        #############################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E-ro26tdUOIy"},"outputs":[],"source":["model = UNet(n_channels=3, n_classes=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wE6fveqlUOIy"},"outputs":[],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"ncudHa0UJSgQ","papermill":{"duration":0.031715,"end_time":"2021-02-18T10:06:32.670851","exception":false,"start_time":"2021-02-18T10:06:32.639136","status":"completed"},"tags":[]},"source":["# Training\n","\n","In this section, you will implement some functions in your training loop. There are several crucial functions you need to implement:\n","\n","- Pixel Accuracy: Pixel Accuracy measures the percentage of correctly predicted pixels out of the total pixels in the image\n","\n","- mIoU (Mean Intersection over Union): mIoU evaluates the spatial overlap between the predicted and ground truth segmentation masks for multiple classes.\n","\n","- Dice Loss: Dice Loss quantifies the dissimilarity between the predicted and ground truth masks, emphasizing the agreement between the two masks\n","\n","Try to use labeled data design and train a segmentor (Unet) from scratch to predict the mask of a breast cancer lesions image."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T09:51:51.219871Z","iopub.status.busy":"2023-10-07T09:51:51.219382Z","iopub.status.idle":"2023-10-07T09:51:51.229928Z","shell.execute_reply":"2023-10-07T09:51:51.229293Z","shell.execute_reply.started":"2023-10-07T09:51:51.219835Z"},"papermill":{"duration":0.039711,"end_time":"2021-02-18T10:06:32.742919","exception":false,"start_time":"2021-02-18T10:06:32.703208","status":"completed"},"tags":[],"trusted":true,"id":"lRo1sBYiUOIy"},"outputs":[],"source":["def pixel_accuracy(output, mask):\n","    with torch.no_grad():\n","        #############################################################################################################\n","        #  1. Convert the output to class predictions using argmax after applying softmax                           #\n","        #  2. Create a tensor of binary values indicating correct predictions                                       #\n","        #  3. Calculate accuracy by dividing the number of correct predictions by the total number of predictions   #\n","        #############################################################################################################\n","        ...\n","        #############################################################################################################\n","        #                                      End of your code                                                     #\n","        #############################################################################################################\n","    return accuracy"]},{"cell_type":"markdown","metadata":{"id":"5TcmdRkKUOIy"},"source":["## mIoU\n","\n","mean Intersection over Union (mIoU), a metric used to evaluate the performance of a segmentation model. It takes predicted masks and ground truth masks as input, computes the IoU for each class, and returns the average IoU across all classes.\n","\n","![image](https://hackmd.io/_uploads/SkLIbhFL6.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T09:51:51.231937Z","iopub.status.busy":"2023-10-07T09:51:51.231409Z","iopub.status.idle":"2023-10-07T09:51:51.241722Z","shell.execute_reply":"2023-10-07T09:51:51.240971Z","shell.execute_reply.started":"2023-10-07T09:51:51.231895Z"},"id":"5cqfcLtOJSgS","papermill":{"duration":0.048322,"end_time":"2021-02-18T10:06:32.826838","exception":false,"start_time":"2021-02-18T10:06:32.778516","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def mIoU(pred_mask, mask, smooth=1e-10, n_classes=3):\n","    with torch.no_grad():\n","        pred_mask = F.softmax(pred_mask, dim=1)\n","        pred_mask = torch.argmax(pred_mask, dim=1)\n","        pred_mask = pred_mask.contiguous().view(-1)\n","        mask = mask.contiguous().view(-1)\n","\n","        iou_per_class = []\n","        for clas in range(0, n_classes): #loop per pixel class\n","            true_class = pred_mask == clas\n","            true_label = mask == clas\n","\n","            if true_label.long().sum().item() == 0: #no exist label in this loop\n","                iou_per_class.append(np.nan)\n","            else:\n","\n","                #############################################################################################################\n","                #  Calculate the intersection of the true positive pixels for the current class                             #\n","                #  Calculate the union of the true positive and false negative pixels for the current class                 #\n","                #############################################################################################################\n","                intersect = ...\n","                union = ...\n","                #############################################################################################################\n","                #                                      End of your code                                                     #\n","                #############################################################################################################\n","                iou = (intersect + smooth) / (union +smooth)\n","                iou_per_class.append(iou)\n","        return np.nanmean(iou_per_class)"]},{"cell_type":"markdown","metadata":{"id":"VaKA_A-nUOIz"},"source":["## Dice Loss\n","\n","Dice loss is based on Sørensen-Dice coefficient. It measures the overlap between the predicted and target segmentation masks. Dice loss provides a differentiable and smooth measure of segmentation accuracy.\n","\n","${DiceLoss}(y, \\bar p) = 1 - \\cfrac{(2y\\bar p + \\epsilon)}{(y + \\bar p + \\epsilon)}$\n","\n","- $y$ represents the ground truth mask.\n","- $\\bar p$ represents the predicted mask.\n","- $\\epsilon$ is a small constant added for numerical stability."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"377VJ1yrUOIz"},"outputs":[],"source":["class DiceLoss(nn.Module):\n","    \"\"\"\n","    Dice loss\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, eps=1e-6):\n","        \"\"\"\n","        Calculation of dice loss\n","\n","        :param inputs: model predictions\n","        :param targets: target values\n","        :param eps: stability factor, defaults to 1e-6\n","        :return: loss value\n","        \"\"\"\n","        #\n","        #######################################\n","        #        implement dice loss          #\n","        #######################################\n","        ...\n","        #######################################\n","        #          End of your code           #\n","        #######################################\n","        return 1.0 - dice.mean()"]},{"cell_type":"markdown","metadata":{"id":"lROoUv_pUOIz"},"source":["# Training loop\n","\n","Call train function in a loop.  \n","Take a break and wait."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T09:51:51.24363Z","iopub.status.busy":"2023-10-07T09:51:51.243068Z","iopub.status.idle":"2023-10-07T09:51:51.262225Z","shell.execute_reply":"2023-10-07T09:51:51.26144Z","shell.execute_reply.started":"2023-10-07T09:51:51.243596Z"},"id":"8i9UIFUJJSgZ","papermill":{"duration":0.060048,"end_time":"2021-02-18T10:06:32.922403","exception":false,"start_time":"2021-02-18T10:06:32.862355","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","def fit(epochs, model, train_loader, val_loader, criterion1, criterion2, optimizer, scheduler, patch=False):\n","    torch.cuda.empty_cache()\n","    train_losses = []\n","    test_losses = []\n","    val_iou = []; val_acc = []\n","    train_iou = []; train_acc = []\n","    lrs = []\n","    min_loss = np.inf\n","    decrease = 1 ; not_improve=0\n","\n","    model.to(device)\n","    fit_time = time.time()\n","    for e in range(epochs):\n","        since = time.time()\n","        running_loss = 0\n","        iou_score = 0\n","        accuracy = 0\n","        #training loop\n","        model.train()\n","        for i, data in enumerate(tqdm(train_loader)):\n","            #training phase\n","            ########################################################################\n","            # TODO: Forward, backward and optimize                                 #\n","            # 1. process input through the network                                 #\n","            # 2. compute the two losses                                            #\n","            # 3. caluate mIoU and piexl accuracy                                   #\n","            # 4. propagate gradients back into the network’s parameters            #\n","            # 5. update the weights of the network                                 #\n","            # 6. reset gradient                                                    #\n","            ########################################################################\n","            image, mask = data\n","\n","            image = ...\n","            mask = ...\n","            output = ...\n","            loss = ...\n","            iou_score += ...\n","            accuracy += ...\n","            ########################################################################\n","            #                           End of your code                           #\n","            ########################################################################\n","            #step the learning rate\n","            lrs.append(get_lr(optimizer))\n","            scheduler.step()\n","\n","            running_loss += loss.item()\n","\n","        else:\n","            model.eval()\n","            test_loss = 0\n","            test_accuracy = 0\n","            val_iou_score = 0\n","            #validation loop\n","            with torch.no_grad():\n","                for i, data in enumerate(tqdm(val_loader)):\n","                    ########################################################################\n","                    # TODO: Forward, backward and optimize                                 #\n","                    # 1. process input through the network                                 #\n","                    # 2. compute the two losses                                            #\n","                    # 3. caluate mIoU and piexl accuracy                                   #\n","                    # 4. save this batch's loss into test_loss                             #\n","                    ########################################################################\n","                    image, mask = data\n","\n","                    image = ...\n","                    mask = ...\n","                    output = ...\n","                    val_iou_score += ...\n","                    test_accuracy += ...\n","                    loss = ...\n","                    test_loss += ...\n","                    ########################################################################\n","                    #                           End of your code                           #\n","                    ########################################################################\n","\n","            #calculatio mean for each batch\n","            train_losses.append(running_loss/len(train_loader))\n","            test_losses.append(test_loss/len(val_loader))\n","\n","\n","            if min_loss > (test_loss/len(val_loader)):\n","                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss/len(val_loader))))\n","                min_loss = (test_loss/len(val_loader))\n","                decrease += 1\n","                if decrease % 5 == 0:\n","                    print('saving model...')\n","                    torch.save(model, 'Unet-Mobilenet_v2_mIoU-{:.3f}.pt'.format(val_iou_score/len(val_loader)))\n","\n","\n","            if (test_loss/len(val_loader)) > min_loss:\n","                not_improve += 1\n","                min_loss = (test_loss/len(val_loader))\n","                print(f'Loss Not Decrease for {not_improve} time')\n","                if not_improve == 7:\n","                    print('Loss not decrease for 7 times, Stop Training')\n","                    break\n","\n","            #iou\n","            val_iou.append(val_iou_score/len(val_loader))\n","            train_iou.append(iou_score/len(train_loader))\n","            train_acc.append(accuracy/len(train_loader))\n","            val_acc.append(test_accuracy/ len(val_loader))\n","            print(\"Epoch:{}/{}..\".format(e+1, epochs),\n","                  \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n","                  \"Val Loss: {:.3f}..\".format(test_loss/len(val_loader)),\n","                  \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n","                  \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n","                  \"Train Acc:{:.3f}..\".format(accuracy/len(train_loader)),\n","                  \"Val Acc:{:.3f}..\".format(test_accuracy/len(val_loader)),\n","                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n","\n","    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n","               'train_miou' :train_iou, 'val_miou':val_iou,\n","               'train_acc' :train_acc, 'val_acc':val_acc,\n","               'lrs': lrs}\n","    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T09:51:51.263871Z","iopub.status.busy":"2023-10-07T09:51:51.263388Z","iopub.status.idle":"2023-10-07T10:59:19.931143Z","shell.execute_reply":"2023-10-07T10:59:19.930125Z","shell.execute_reply.started":"2023-10-07T09:51:51.263837Z"},"id":"vo-VQwfJJSgg","papermill":{"duration":3881.864248,"end_time":"2021-02-18T11:11:14.819081","exception":false,"start_time":"2021-02-18T10:06:32.954833","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["################################################################################\n","#     You can adjust those hyper parameters to loop for max_epochs times       #\n","################################################################################\n","max_lr = 1e-3\n","epoch = 10\n","weight_decay = 1e-4\n","\n","criterion1 = nn.CrossEntropyLoss()\n","criterion2 = DiceLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n","sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n","                                            steps_per_epoch=len(train_loader))\n","\n","history = fit(epoch, model, train_loader, val_loader, criterion1, criterion2, optimizer, sched)\n","################################################################################\n","#                               End of your code                               #\n","################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T10:59:19.933339Z","iopub.status.busy":"2023-10-07T10:59:19.932737Z","iopub.status.idle":"2023-10-07T10:59:20.020877Z","shell.execute_reply":"2023-10-07T10:59:20.01997Z","shell.execute_reply.started":"2023-10-07T10:59:19.933299Z"},"id":"km58PT8qvJ4g","papermill":{"duration":0.15037,"end_time":"2021-02-18T11:11:15.01764","exception":false,"start_time":"2021-02-18T11:11:14.86727","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["torch.save(model, 'Unet.pt')"]},{"cell_type":"markdown","metadata":{"id":"3fmIJLnCUOI0"},"source":["#### Visualize accuracy and loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T10:59:20.022607Z","iopub.status.busy":"2023-10-07T10:59:20.022258Z","iopub.status.idle":"2023-10-07T10:59:20.030974Z","shell.execute_reply":"2023-10-07T10:59:20.029721Z","shell.execute_reply.started":"2023-10-07T10:59:20.02257Z"},"id":"ojw74huJJSgn","papermill":{"duration":0.094126,"end_time":"2021-02-18T11:11:15.563574","exception":false,"start_time":"2021-02-18T11:11:15.469448","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def plot_loss(history):\n","    plt.plot(history['val_loss'], label='val', marker='o')\n","    plt.plot( history['train_loss'], label='train', marker='o')\n","    plt.title('Loss per epoch'); plt.ylabel('loss');\n","    plt.xlabel('epoch')\n","    plt.legend(), plt.grid()\n","    plt.show()\n","\n","def plot_score(history):\n","    plt.plot(history['train_miou'], label='train_mIoU', marker='*')\n","    plt.plot(history['val_miou'], label='val_mIoU',  marker='*')\n","    plt.title('Score per epoch'); plt.ylabel('mean IoU')\n","    plt.xlabel('epoch')\n","    plt.legend(), plt.grid()\n","    plt.show()\n","\n","def plot_acc(history):\n","    plt.plot(history['train_acc'], label='train_accuracy', marker='*')\n","    plt.plot(history['val_acc'], label='val_accuracy',  marker='*')\n","    plt.title('Accuracy per epoch'); plt.ylabel('Accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(), plt.grid()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T10:59:20.032698Z","iopub.status.busy":"2023-10-07T10:59:20.032188Z","iopub.status.idle":"2023-10-07T10:59:20.516335Z","shell.execute_reply":"2023-10-07T10:59:20.515198Z","shell.execute_reply.started":"2023-10-07T10:59:20.032662Z"},"id":"KT-pRvoJJSgq","papermill":{"duration":0.71827,"end_time":"2021-02-18T11:11:16.361999","exception":false,"start_time":"2021-02-18T11:11:15.643729","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["plot_loss(history)\n","plot_score(history)\n","plot_acc(history)"]},{"cell_type":"markdown","metadata":{"id":"akW4OZHfUOI-"},"source":["## Predict Result\n","\n","預測`test`並將結果上傳至[**Kaggle**](https://www.kaggle.com/t/13040d09b42940ba90f96cca324ee90d)\n","\n","執行完畢此區的程式碼後，會將`test`預測完的結果存下來。\n","\n","上傳流程\n","1. 點選左側選單最下方的資料夾圖示\n","2. 右鍵「output.csv」\n","3. 點選「Download」\n","4. 至連結網頁點選「Submit Predictions」\n","5. 將剛剛下載的檔案上傳\n","6. 系統會計算並公布其中50%資料的正確率"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xFzjGJTUOI-"},"outputs":[],"source":["TEST_IMAGE_PATH = '/BCSS/test/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMe2zYCPUOI-"},"outputs":[],"source":["test_df = create_df(TEST_IMAGE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iCdZLJ8ZUOI-"},"outputs":[],"source":["X_test = test_df['id'].to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HejSiUnUOI-"},"outputs":[],"source":["model = torch.load('Unet.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RvYXuqMjUOI-"},"outputs":[],"source":["class BCSSTestDataset(Dataset):\n","\n","    def __init__(self, img_path, X, transform=None):\n","        self.img_path = img_path\n","        self.X = X\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        img = cv2.imread(self.img_path + self.X[idx] + '.png')\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","        if self.transform is not None:\n","            aug = self.transform(image=img, mask=mask)\n","            img = Image.fromarray(aug['image'])\n","\n","        if self.transform is None:\n","            img = Image.fromarray(img)\n","\n","        return img\n","\n","test_set = BCSSTestDataset(TEST_IMAGE_PATH, X_test)\n","print(len(test_set))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dSbzFBtwUOI_"},"outputs":[],"source":["def predict_image(model, image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n","    model.eval()\n","    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n","    image = t(image)\n","    model.to(device); image=image.to(device)\n","    with torch.no_grad():\n","\n","        image = image.unsqueeze(0)\n","\n","        output = model(image)\n","        masked = torch.argmax(output, dim=1)\n","        masked = masked.cpu().squeeze(0)\n","    return masked"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YoYo9UuCUOI_"},"outputs":[],"source":["import pandas as pd\n","from tqdm import tqdm\n","\n","data = []\n","\n","for i in tqdm(range(len(test_set))):\n","    img = test_set[i]\n","    pred_mask = predict_image(model, img)\n","    index = i\n","\n","    data.append({'index': index, 'pred_mask': pred_mask.numpy().tolist()})\n","\n","df = pd.DataFrame(data)\n","\n","df.to_csv('output.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":333968,"sourceId":1834160,"sourceType":"datasetVersion"}],"dockerImageVersionId":30061,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}