{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Z1Snuk7rIK"
      },
      "source": [
        "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSwr9MgZogRZ"
      },
      "source": [
        "Before we start, please put your name and SID in following format: <br>\n",
        ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DzsjuDhlz_k"
      },
      "source": [
        "**Your Answer:**   \n",
        "Hi I'm 陳文薇, B094020007"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d-Zzebq7rIM"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc9gd_Wk7rIN"
      },
      "source": [
        "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
        "\n",
        "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
        "\n",
        "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
        "\n",
        "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWnFRBjUQh5-"
      },
      "source": [
        "# Notice\n",
        "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
        "\n",
        "You can use BERT and RoBERTa encoder model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_HGEhMwnTn0_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giUId1Naqacs",
        "tags": []
      },
      "source": [
        "##  Versions of used packages\n",
        "\n",
        "We will check PyTorch version to make sure everything work properly.  \n",
        "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
        "This is the default version in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Vuw-gNvjqcYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef3bb77-6a00-4a84-9a06-7093885632c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "torch 2.1.0+cu121\n",
            "torchvision 0.16.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "print('python', sys.version.split('\\n')[0])\n",
        "print('torch', torch.__version__)\n",
        "print('torchvision', torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2YbY8FGQh5_"
      },
      "source": [
        "# Task 1: Text Sentiment Classification (40 points)\n",
        "\n",
        "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4s_a5D7rIR"
      },
      "source": [
        "## Loading Model and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPUkTbnL7rIR"
      },
      "source": [
        "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
        "\n",
        "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rK0ouXa09pDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d17c26-bfe5-4317-c030-cbc05cf3b253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy installation\n",
            "pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (1.60.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.17.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.5.1)\n",
            "Requirement already satisfied: protobuf==3.9.2 in /usr/local/lib/python3.10/dist-packages (3.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from protobuf==3.9.2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from protobuf==3.9.2) (1.16.0)\n",
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.10/dist-packages (2.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.34.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.4 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.34.4)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.4->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.4->boto3) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# you might need some additional installations there\n",
        "!echo happy installation\n",
        "!pip -V\n",
        "!pip install grpcio\n",
        "!pip install google-auth\n",
        "!pip install protobuf==3.9.2\n",
        "!pip install pyprind\n",
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "dmGCAevi7rIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b538fea-80bc-4b35-aa30-abbc6b91dd91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "#########################################################################\n",
        "#            Loading tokenizer and model from transformer               #\n",
        "#########################################################################\n",
        "# from transformers import xxx\n",
        "from transformers import AutoTokenizer, AutoConfig, BertForSequenceClassification\n",
        "\n",
        "bert_type = 'bert-base-uncased'\n",
        "\n",
        "\n",
        "# ---------- 1. load from torch.hub ----------\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', bert_type)\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model = torch.hub.load('huggingface/pytorch-transformers', 'model', bert_type)\n",
        "\n",
        "# ---------- 2. load from installed huggingface ----------\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model = BertForSequenceClassification.from_pretrained(bert_type)\n",
        "\n",
        "\n",
        "\n",
        "# finetune from the output from bert to your task\n",
        "model.classifier = nn.Linear(768, 3, bias=True)\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "#########################################################################\n",
        "#            Loading tokenizer and model from transformer               #\n",
        "#########################################################################\n",
        "# from transformers import xxx\n",
        "from transformers import AutoTokenizer, AutoConfig, RobertaForSequenceClassification\n",
        "\n",
        "bert_type = 'roberta-base'\n",
        "\n",
        "\n",
        "# ---------- 1. load from torch.hub ----------\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', bert_type)\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model = torch.hub.load('huggingface/pytorch-transformers', 'model', bert_type)\n",
        "\n",
        "# ---------- 2. load from installed huggingface ----------\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model = RobertaForSequenceClassification.from_pretrained(bert_type)\n",
        "\n",
        "# finetune from the output from bert to your task\n",
        "model.classifier = nn.Linear(768, 3, bias=True)\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC5VvGJ6Gy_-",
        "outputId": "ea7edb3c-5fd2-41e3-d3cb-870f8b9ee776"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMThsYeDa2O"
      },
      "source": [
        "## How to Get Data\n",
        "\n",
        "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
        "\n",
        "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
        "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
        "3. Select the location where you want to place the shortcut.\n",
        "4. Click Add shortcut.\n",
        "\n",
        "After above procedures, we have a shortcut of zip file of dataset.  \n",
        "We can access this in colab after granting the permission of Google Drive.\n",
        "\n",
        "---\n",
        "\n",
        "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lZnFgi5i_2oA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0f732e-3d9c-43d3-d95e-499fa6f1d405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqO8DiB6VRQZ"
      },
      "source": [
        "## Unzip Data\n",
        "\n",
        "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
        "\n",
        "- `train.csv`, `test.csv` and `val.csv`\n",
        "\n",
        "Training set 有 **10248** 筆資料.  \n",
        "Validation set 有 **1317** 筆資料.  \n",
        "Testing set 有 **3075** 筆資料.  \n",
        "\n",
        "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OSlTMdxf8Zd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d83caef-ce62-4f0b-9905-9a3a4f17f093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace val.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSQuEBjlQh6C"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wf5GXTme7rIT"
      },
      "outputs": [],
      "source": [
        "# Utility function to extract text and label from csv file\n",
        "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
        "    text_list = []\n",
        "    label_list = []\n",
        "\n",
        "\n",
        "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
        "    with open(f_path) as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for line in reader:\n",
        "            text_list.append(line['text'])\n",
        "            if mode != 'test':\n",
        "                label_list.append(int(line['sentiment_label']))\n",
        "    print(label_list)\n",
        "    return text_list, label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6fpY0ZrK7rIV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, f_name='.', mode='train'):\n",
        "        self.mode = mode\n",
        "        print(f_name)\n",
        "        text_list, label_list = get_texts(f_name, mode)\n",
        "        print('mode', mode, 'has', len(text_list), 'datas')\n",
        "        text_list = tokenizer(text_list,\n",
        "                             truncation=True, padding=True,\n",
        "                             return_tensors='pt')\n",
        "\n",
        "        self.text_list = text_list['input_ids']\n",
        "        self.mask_list = text_list['attention_mask']\n",
        "\n",
        "        self.label_list = label_list\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_list[idx]\n",
        "        mask = self.mask_list[idx]\n",
        "        if self.mode == 'test':\n",
        "            return text, mask\n",
        "        label = torch.tensor(self.label_list[idx])\n",
        "        return text, mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6xi-ovAQh6C"
      },
      "source": [
        "## `DataLoader`\n",
        "\n",
        "`torch.utils.data.DataLoader` define how to sample from `dataset` and some other function like:\n",
        "+ `shuffle` : set to `True` to have the data reshuffled at every epoch\n",
        "+ `batch_size` : how many samples per batch to load\n",
        "\n",
        "See [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for more details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nCmM4FSw7rIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29cfbd5e-807e-4607-a7ab-f5318f5e549e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "[0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 2, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 0, 1, 0, 0, 1, 2, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 1, 0, 1, 2, 1, 0, 0, 1, 0, 0, 1, 2, 2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 2, 0, 2, 0, 2, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 1, 0, 0, 1, 1, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 1, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 1, 1, 1, 1, 0, 2, 2, 0, 0, 2, 1, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 1, 2, 2, 0, 2, 1, 1, 0, 2, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 2, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 2, 1, 2, 2, 0, 0, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 2, 2, 2, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 1, 0, 0, 2, 2, 1, 0, 0, 1, 2, 0, 0, 1, 2, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 2, 1, 0, 0, 2, 0, 1, 1, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 2, 1, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 2, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 1, 0, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 1, 2, 0, 0, 1, 0, 2, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 2, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 2, 1, 0, 2, 1, 2, 1, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 2, 0, 1, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 2, 0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 2, 0, 2, 0, 1, 1, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 2, 1, 2, 1, 1, 1, 2, 0, 0, 1, 1, 1, 2, 2, 0, 2, 1, 0, 1, 0, 0, 0, 1, 0, 2, 2, 2, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 2, 2, 0, 1, 0, 2, 0, 1, 2, 0, 0, 1, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 0, 2, 1, 1, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 2, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 1, 1, 0, 2, 0, 2, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 2, 2, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 2, 2, 0, 1, 2, 1, 2, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 2, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 1, 1, 2, 0, 1, 0, 0, 0, 2, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 2, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 2, 0, 1, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 2, 2, 2, 1, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 1, 2, 0, 0, 2, 0, 1, 1, 0, 2, 0, 1, 0, 0, 2, 0, 1, 2, 0, 1, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 2, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 2, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 2, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 1, 1, 0, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 2, 1, 0, 0, 0, 1, 0, 2, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 2, 2, 1, 1, 2, 0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 2, 1, 1, 0, 2, 0, 0, 1, 0, 1, 2, 0, 2, 0, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 1, 2, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 0, 2, 1, 2, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 2, 0, 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 1, 2, 0, 0, 1, 2, 1, 0, 0, 2, 1, 0, 2, 2, 2, 1, 2, 2, 0, 0, 0, 1, 2, 2, 0, 1, 0, 0, 0, 2, 0, 1, 2, 0, 1, 2, 0, 1, 1, 2, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 2, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 1, 0, 1, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 0, 0, 2, 0, 2, 1, 0, 1, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 0, 2, 0, 1, 2, 2, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 2, 0, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 1, 1, 2, 1, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 2, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 2, 0, 2, 1, 2, 1, 1, 1, 0, 2, 1, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 2, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 0, 0, 0, 0, 2, 1, 2, 0, 2, 1, 0, 1, 2, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 2, 0, 1, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 2, 2, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 2, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 1, 1, 2, 0, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 1, 2, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 1, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 2, 2, 1, 2, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 2, 0, 0, 1, 1, 2, 2, 0, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 1, 2, 2, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 1, 0, 1, 2, 0, 0, 2, 0, 2, 1, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 1, 2, 2, 1, 0, 2, 2, 0, 0, 1, 0, 0, 2, 0, 1, 0, 1, 2, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 2, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 0, 2, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 2, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 1, 2, 0, 0, 0, 2, 0, 2, 0, 1, 2, 2, 0, 1, 2, 0, 0, 2, 0, 1, 1, 2, 1, 2, 1, 0, 2, 0, 0, 1, 0, 2, 2, 0, 1, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 2, 0, 1, 0, 1, 2, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 2, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 0, 2, 1, 0, 0, 1, 0, 1, 0, 2, 0, 2, 0, 2, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 2, 1, 0, 1, 2, 1, 1, 0, 1, 0, 0, 0, 0, 2, 2, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 2, 2, 2, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 1, 2, 2, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 2, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 1, 0, 0, 1, 0, 2, 2, 0, 2, 2, 1, 0, 1, 0, 1, 0, 1, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 2, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0, 2, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 2, 1, 0, 2, 0, 0, 1, 0, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 2, 0, 0, 0, 1, 1, 2, 0, 1, 0, 1, 2, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 2, 2, 0, 2, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 0, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 2, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 2, 1, 1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 2, 1, 0, 1, 2, 0, 2, 0, 0, 2, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1, 1, 0, 1, 1, 2, 2, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 2, 1, 0, 2, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 2, 0, 2, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 2, 0, 1, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 1, 2, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 2, 0, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 1, 0, 2, 1, 2, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 2, 1, 1, 2, 0, 0, 2, 2, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 2, 1, 2, 0, 1, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 0, 1, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 0, 2, 0, 0, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 2, 0, 0, 1, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 1, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 2, 1, 2, 2, 0, 0, 0, 0, 0, 1, 2, 0, 2, 1, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 2, 2, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 0, 2, 1, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 2, 0, 1, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 2, 1, 0, 2, 1, 1, 2, 2, 1, 2, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 1, 2, 0, 0, 1, 0, 1, 1, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 0, 1, 2, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 2, 1, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 2, 1, 2, 0, 1, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 2, 0, 0, 1, 1, 0, 1, 2, 0, 0, 2, 1, 0, 0, 2, 2, 0, 1, 0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 2, 0, 2, 1, 0, 2, 2, 0, 2, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 2, 2, 1, 1, 1, 0, 2, 1, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 2, 2, 0, 2, 0, 1, 0, 1, 2, 0, 0, 1, 1, 0, 1, 2, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 2, 2, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 1, 1, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 2, 0, 1, 2, 2, 2, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 1, 2, 1, 2, 0, 2, 1, 0, 0, 2, 2, 0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 2, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 2, 1, 1, 2, 2, 0, 1, 1, 0, 1, 0, 2, 2, 2, 2, 1, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 1, 1, 0, 1, 0, 2, 2, 0, 0, 1, 1, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 2, 2, 0, 1, 2, 0, 0, 0, 2, 0, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 1, 2, 0, 2, 0, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 1, 1, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 2, 2, 1, 1, 0, 2, 1, 0, 0, 2, 2, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 2, 0, 0, 1, 0, 1, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 2, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 2, 0, 1, 1, 2, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 0, 2, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 2, 1, 2, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 2, 0, 2, 0, 1, 1, 0, 0, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 2, 1, 0, 2, 0, 2, 0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 0, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 2, 2, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 2, 0, 1, 2, 0, 2, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 1, 0, 1, 1, 1, 2, 2, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 1, 2, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 2, 2, 1, 0, 0, 2, 1, 0, 2, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 1, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 0, 1, 0, 0, 2, 1, 1, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 2, 0, 0, 0, 1, 2, 2, 0, 2, 0, 0, 1, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 2, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 2, 0, 0, 1, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 2, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 2, 2, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 2, 0, 2, 0, 0, 2, 1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 2, 1, 1, 0, 1, 0, 2, 2, 0, 0, 2, 0, 2, 0, 1, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 2, 1, 2, 2, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 1, 2, 1, 1, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 1, 0, 2, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 1, 0, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 2, 2, 1, 0, 0, 2, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 1, 0, 1, 0, 2, 2, 0, 0, 1, 0, 0, 2, 0, 0, 2, 1, 0, 0, 1, 1, 0, 2, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 1, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 0, 1, 1, 2, 0, 2, 0, 2, 1, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 2, 0, 2, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 2, 2, 1, 0, 2, 2, 0, 0, 1, 2, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 1, 0, 0, 2, 0, 0, 1, 0, 1, 0, 2, 2, 1, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 1, 0, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 2, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 2, 1, 1, 0, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 1, 2, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 2, 2, 1, 2, 0, 1, 0, 2, 0, 0, 1, 0, 2, 1, 2, 0, 2, 0, 2, 0, 0, 0, 2, 0, 2, 1, 1, 1, 0, 1, 0, 0, 2, 0, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2, 2, 0, 1, 0, 0, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 1, 2, 0, 1, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 2, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 0, 2, 0, 2, 2, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 1, 2, 2, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 2, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 2, 1, 0, 2, 0, 0, 1, 2, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 2, 1, 2, 2, 1, 0, 0, 0, 0, 1, 2, 2, 0, 2, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 2, 2, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 1, 0, 2, 0, 0, 1, 2, 2, 0, 0, 1, 0, 1, 1, 0, 0, 1, 2, 2, 0, 0, 1, 0, 0, 0, 2, 0, 1, 2, 2, 1, 0, 1, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 2, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 2, 1]\n",
            "mode train has 10248 datas\n",
            ".\n",
            "[0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 1, 1, 0, 0, 2, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 2, 0, 1, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 2, 0, 1, 0, 1, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 2, 1, 1, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 2, 1, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 2, 1, 0, 2, 0, 0, 2, 0, 1, 0, 2, 0, 1, 0, 2, 2, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 2, 1, 0, 2, 0, 0, 1, 0, 2, 1, 1, 1, 2, 0, 2, 2, 0, 0, 1, 2, 0, 0, 0, 0, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 1, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 2, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 2, 2, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 1, 1, 1, 2, 0, 0, 1, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 1, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 1, 1, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 1, 2, 2, 2, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 2, 1, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 2, 2, 0, 1, 0, 2, 0, 0, 1, 0, 2, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 0, 2, 0, 0, 2, 0, 0, 2, 1, 0, 2, 0, 2, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 2, 2, 0, 0, 1, 0, 1, 2, 2, 0, 2, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 0, 2, 1, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 2, 0, 2, 0, 1, 2, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 2, 2, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n",
            "mode val has 1317 datas\n",
            ".\n",
            "[]\n",
            "mode test has 3075 datas\n"
          ]
        }
      ],
      "source": [
        "dataset_train = TwitterDataset(mode='train')\n",
        "dataset_val = TwitterDataset(mode='val')\n",
        "dataset_test = TwitterDataset(mode='test')\n",
        "\n",
        "batch_size = 16\n",
        "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
        "                       shuffle=True)\n",
        "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
        "                       shuffle=False)\n",
        "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
        "                       shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bqkvofHc7rIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f8d80c-79a5-4613-84b3-05b524643a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token ['<s>', '@', 'united', 'ĠI', 'Ġhave', 'Ġnever', 'Ġbeen', 'Ġmislead', 'Ġby', 'Ġa', 'Ġcompany', 'Ġas', 'Ġmany', 'Ġtimes', 'Ġas', 'ĠI', 'Ġhave', 'Ġthis', 'Ġweek', 'Ġby', 'ĠUnited', 'ĠAirlines', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "token to s <s>@united I have never been mislead by a company as many times as I have this week by United Airlines!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
          ]
        }
      ],
      "source": [
        "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0]) # converts a sequence of numeric IDs in the training dataset into their corresponding tokens using the specified tokenizer.\n",
        "print('token', t)\n",
        "print('token to s', tokenizer.convert_tokens_to_string(t)) # converts a sequence of tokens (t) back into the original text string using the specified tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMZFYqQ3Qh6D"
      },
      "source": [
        "# Define loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DxZrfCqW7rIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14933e9-120c-4934-b7c5-ebfed4a2ae40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "from torch import nn\n",
        "from transformers import AdamW\n",
        "\n",
        "\n",
        "w_decay = 0.01\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=w_decay)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpwgE2Gd7rIZ"
      },
      "source": [
        "# Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zlaiAZAD7rIa"
      },
      "outputs": [],
      "source": [
        "def accuracy(raw_preds, y):\n",
        "    preds = raw_preds.argmax(dim=1)\n",
        "    acc = (preds == y).sum()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtJXdIO7Qh6D"
      },
      "source": [
        "# Train function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT**"
      ],
      "metadata": {
        "id": "8-o9gu4LpkBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Testing process                              #\n",
        "        #########################################################################\n",
        "        # 1. Clean the gradients of optimizer\n",
        "        # 2. Put correct variables into model\n",
        "        # 3. Get prediction\n",
        "        # 4. Evalutate by criterion and accuracy\n",
        "\n",
        "        # 1. Clean the gradients of optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # 2. Put correct variables into model\n",
        "        outputs = model(text, mask)\n",
        "        #outputs = torch.tensor(outputs).to(device)\n",
        "        # 3. Get prediction\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        # 4. Evalutate by criterion and accuracy\n",
        "        loss = criterion(outputs.logits, label)\n",
        "        acc = accuracy(outputs.logits, label)\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "def test(model, data, criterion, log_loss=False):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Training process                             #\n",
        "        #########################################################################\n",
        "        # 1. Put correct variables into model\n",
        "        # 2. Get prediction\n",
        "        # 3. Evaluate by criterion and accuracy\n",
        "        with torch.no_grad():\n",
        "          # 1. Put correct variables into model\n",
        "          outputs = model(text, mask)\n",
        "          #outputs = torch.tensor(outputs).to(device)\n",
        "          # 2. Get prediction\n",
        "          predictions = torch.argmax(outputs.logits, dim=1)\n",
        "          # 3. Evaluate by criterion and accuracy\n",
        "          loss = criterion(outputs.logits, label)\n",
        "          acc = accuracy(outputs.logits, label)\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        if log_loss:\n",
        "            val_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "# class for monitoring train and test acc/loss\n",
        "class Meter:\n",
        "    def __init__(self):\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.val_loss_list = []\n",
        "        self.val_acc_list = []\n",
        "\n",
        "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
        "        self.train_loss_list.append(train_loss)\n",
        "        self.train_acc_list.append(train_acc)\n",
        "        self.val_loss_list.append(val_loss)\n",
        "        self.val_acc_list.append(val_acc)\n",
        "\n",
        "    def plot(self):\n",
        "        x = range(len(self.train_loss_list))\n",
        "        plt.plot(x, self.train_loss_list)\n",
        "        plt.plot(x, self.val_loss_list, color='r')\n",
        "        plt.legend(['train_loss', 'val_loss'])\n",
        "        plt.show()\n",
        "        plt.plot(x, self.train_acc_list)\n",
        "        plt.plot(x, self.val_acc_list, color='r')\n",
        "        plt.legend(['train_acc', 'val_acc'])\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "NIgbsgxQPsEV"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RoBERTa**"
      ],
      "metadata": {
        "id": "HQDOemXPppyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dmc_Gms97rIa"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "        #########################################################################\n",
        "        #                          Testing process                              #\n",
        "        #########################################################################\n",
        "        # 1. Clean the gradients of optimizer\n",
        "        # 2. Put correct variables into model\n",
        "        # 3. Get prediction\n",
        "        # 4. Evalutate by criterion and accuracy\n",
        "        # 1. Clean the gradients of optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # 2. Put correct variables into model\n",
        "        outputs = model(text, mask)\n",
        "        #outputs = torch.tensor(outputs).to(device)\n",
        "        # 3. Get prediction\n",
        "        predictions = torch.argmax(outputs.logits[:, 0, :], dim=1)\n",
        "        # 3. Evaluate by criterion and accuracy\n",
        "        loss = criterion(outputs.logits[:, 0, :], label)\n",
        "        acc = accuracy(outputs.logits[:, 0, :], label)\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "def test(model, data, criterion, log_loss=False):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Training process                             #\n",
        "        #########################################################################\n",
        "        # 1. Put correct variables into model\n",
        "        # 2. Get prediction\n",
        "        # 3. Evaluate by criterion and accuracy\n",
        "        with torch.no_grad():\n",
        "          # 1. Put correct variables into model\n",
        "          outputs = model(text, mask)\n",
        "          #outputs = torch.tensor(outputs).to(device)\n",
        "          # 2. Get prediction\n",
        "          predictions = torch.argmax(outputs.logits[:, 0, :], dim=1)\n",
        "          # 3. Evaluate by criterion and accuracy\n",
        "          loss = criterion(outputs.logits[:, 0, :], label)\n",
        "          acc = accuracy(outputs.logits[:, 0, :], label)\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        if log_loss:\n",
        "            val_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "# class for monitoring train and test acc/loss\n",
        "class Meter:\n",
        "    def __init__(self):\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.val_loss_list = []\n",
        "        self.val_acc_list = []\n",
        "\n",
        "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
        "        self.train_loss_list.append(train_loss)\n",
        "        self.train_acc_list.append(train_acc)\n",
        "        self.val_loss_list.append(val_loss)\n",
        "        self.val_acc_list.append(val_acc)\n",
        "\n",
        "    def plot(self):\n",
        "        x = range(len(self.train_loss_list))\n",
        "        plt.plot(x, self.train_loss_list)\n",
        "        plt.plot(x, self.val_loss_list, color='r')\n",
        "        plt.legend(['train_loss', 'val_loss'])\n",
        "        plt.show()\n",
        "        plt.plot(x, self.train_acc_list)\n",
        "        plt.plot(x, self.val_acc_list, color='r')\n",
        "        plt.legend(['train_acc', 'val_acc'])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExZyrKd57rIb"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bVDe-fRe7rIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374de949-9107-490f-8bed-4f1387ab5ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 641/641 [03:58<00:00,  2.69it/s]\n",
            "100%|██████████| 165/165 [00:04<00:00, 34.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 train_loss: 0.03007949732322534 train_acc: 0.8070843091334895\n",
            "Epoch 1 val_loss:  0.05626379969567287 val_acc : 0.8253606681852695\n",
            "---------- e 1 save best model ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 641/641 [03:58<00:00,  2.69it/s]\n",
            "100%|██████████| 165/165 [00:04<00:00, 35.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 train_loss: 0.021535527034154473 train_acc: 0.8735362997658079\n",
            "Epoch 2 val_loss:  0.05032048345456464 val_acc : 0.8542141230068337\n",
            "---------- e 2 save best model ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 641/641 [03:58<00:00,  2.69it/s]\n",
            "100%|██████████| 165/165 [00:04<00:00, 35.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 train_loss: 0.015621527091030738 train_acc: 0.9068110850897736\n",
            "Epoch 3 val_loss:  0.05917879183219439 val_acc : 0.856492027334852\n",
            "---------- e 3 save best model ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 641/641 [03:58<00:00,  2.69it/s]\n",
            "100%|██████████| 165/165 [00:04<00:00, 35.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 train_loss: 0.011228231878529152 train_acc: 0.9374512099921936\n",
            "Epoch 4 val_loss:  0.060886840392773915 val_acc : 0.856492027334852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 641/641 [03:58<00:00,  2.69it/s]\n",
            "100%|██████████| 165/165 [00:04<00:00, 34.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 train_loss: 0.008320112805172858 train_acc: 0.955503512880562\n",
            "Epoch 5 val_loss:  0.06575156990848849 val_acc : 0.8542141230068337\n"
          ]
        }
      ],
      "source": [
        "#########################################################################\n",
        "#                          Hyper-parameters                             #\n",
        "#########################################################################\n",
        "max_epoch = 5\n",
        "log_interval = 1\n",
        "best_acc = 0\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################\n",
        "\n",
        "m = Meter()\n",
        "\n",
        "for epoch in range(1, max_epoch + 1):\n",
        "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
        "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
        "\n",
        "\n",
        "    if epoch % log_interval == 0:\n",
        "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
        "            epoch, train_loss, train_acc\n",
        "        ))\n",
        "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
        "            epoch, val_loss, val_acc\n",
        "        ))\n",
        "\n",
        "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
        "\n",
        "    # model checkpoint\n",
        "    torch.save(model.state_dict(), 'e{}.pt'.format(epoch))\n",
        "    if val_acc > best_acc:\n",
        "        best_model = model\n",
        "        best_acc = val_acc\n",
        "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "gBkm1yjFb9Je"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SmtW58OR7rIc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "outputId": "b55d02f6-e631-4ec7-cbec-fbd99137a4ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot them out\n",
        "m.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcJcHf7n7rId"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Sf5UTlMZ7rId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f2af45-2d1a-4ba1-b861-c7dd68f45125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 385/385 [00:11<00:00, 33.73it/s]\n"
          ]
        }
      ],
      "source": [
        "best_model.eval()\n",
        "\n",
        "total_out = []\n",
        "for text, mask in tqdm(test_data, total=len(test_data)):\n",
        "    text = text.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    output = best_model(text, mask)\n",
        "    pred = output.logits[:, 0, :]\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    total_out.append(pred)\n",
        "\n",
        "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
        "\n",
        "with open('pred.csv', 'w') as f:\n",
        "    f.write('index,sentiment_label\\n')\n",
        "    for i, pred in enumerate(total_out):\n",
        "        f.write('{},{}\\n'.format(i, pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save best model\n",
        "torch.save(best_model.state_dict(), 'bert_model.pt')"
      ],
      "metadata": {
        "id": "00Am4fzONd7J"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vLmiwr-Qh6F"
      },
      "source": [
        "# Task 2: In-Context learning (32 points)\n",
        "\n",
        "In this task, you will learn how to perform sentiment classification using **prompts** without the need for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "etsDAcPNQh6F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pyprind\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnPD63t3Qh6F"
      },
      "source": [
        "# Loading model and setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1st template**"
      ],
      "metadata": {
        "id": "zhCp4JgL6qlP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "-H44jy0GQh6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "744495fe-f5f1-42b7-c029-c846a3dc42b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#########################################################################\n",
        "#         TODO: Design your own template(prefix) and verbalizer         #\n",
        "#########################################################################\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        #zero-shot\n",
        "        #self.prefix = 'It was bad [MASK] sentence.'\n",
        "        #one-shot\n",
        "        #self.prefix = (\n",
        "        #    \"It was bad sentence. @united This has been the WORST flight experience I've ever had.  [SEP] \"\n",
        "        #    \"It was [MASK] sentence.\")\n",
        "        #few-shot\n",
        "        #self.prefix = (\n",
        "        self.prefix = (\n",
        "            \"It was bad sentence. @united This has been the WORST flight experience I've ever had.  [SEP] \"\n",
        "            \"It was neutral sentence. @VirginAmerica Are the hours of operation for the Club at SFO that are posted online current? [SEP] \"\n",
        "            \"It was good sentence. @AmericanAir Thank you for the response, we got it resolved at the counter. [SEP] \"\n",
        "            \"It was [MASK] sentence.\"\n",
        "        )\n",
        "\n",
        "        self.verbalizer = {\n",
        "            'good': 2,\n",
        "            'neutral':1,\n",
        "            'bad': 0,\n",
        "        }\n",
        "\n",
        "        self.max_seq_length = 512\n",
        "        self.batch_size = 32\n",
        "\n",
        "\n",
        "config = Config()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "bert_type = 'bert-base-uncased'\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained(bert_type, num_labels = 3)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "bert_config = BertConfig.from_pretrained(bert_type)\n",
        "\n",
        "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
        "\n",
        "#######################################################################\n",
        "#                        End of your code                             #\n",
        "#######################################################################\n",
        "\n",
        "softmax = nn.Softmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2nd template**"
      ],
      "metadata": {
        "id": "7QAtJERi6uVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################\n",
        "#         TODO: Design your own template(prefix) and verbalizer         #\n",
        "#########################################################################\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        #zero-shot\n",
        "        #self.prefix = 'It was sentence [MASK].'\n",
        "        #one-shot\n",
        "        self.prefix = (\n",
        "            \"It was bad sentence. @united This has been the WORST flight experience I've ever had.  [SEP] \"\n",
        "            \"It was sentence [MASK].\")\n",
        "        #few-shot\n",
        "        #self.prefix = (\n",
        "        #self.prefix = (\n",
        "        #    \"It was bad sentence. @united This has been the WORST flight experience I've ever had.  [SEP] \"\n",
        "        #    \"It was neutral sentence. @VirginAmerica Are the hours of operation for the Club at SFO that are posted online current? [SEP] \"\n",
        "        #    \"It was good sentence. @AmericanAir Thank you for the response, we got it resolved at the counter. [SEP] \"\n",
        "        #    \"It was a sentence [MASK].\"\n",
        "        #)\n",
        "\n",
        "        self.verbalizer = {\n",
        "            'good': 2,\n",
        "            'neutral':1,\n",
        "            'bad': 0,\n",
        "        }\n",
        "\n",
        "        self.max_seq_length = 512\n",
        "        self.batch_size = 32\n",
        "\n",
        "\n",
        "config = Config()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "bert_type = 'bert-base-uncased'\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained(bert_type, num_labels = 3)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "bert_config = BertConfig.from_pretrained(bert_type)\n",
        "\n",
        "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
        "\n",
        "#######################################################################\n",
        "#                        End of your code                             #\n",
        "#######################################################################\n",
        "\n",
        "softmax = nn.Softmax(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25bAfKR46wYx",
        "outputId": "458b2dab-4b75-4821-b720-49e32412099e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3rd template**"
      ],
      "metadata": {
        "id": "xm_WgwUl6-jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################\n",
        "#         TODO: Design your own template(prefix) and verbalizer         #\n",
        "#########################################################################\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        #zero-shot\n",
        "        #self.prefix = 'Describe the feeling: It was sentence [MASK].'\n",
        "        #one-shot\n",
        "        #self.prefix = (\n",
        "        #    \"It was bad sentence. @united This has been the WORST flight experience I've ever had.  [SEP] \"\n",
        "        #    \"Describe the feeling: It was sentence [MASK].\")\n",
        "        #few-shot\n",
        "        #self.prefix = (\n",
        "        self.prefix = (\n",
        "            \"It was bad sentence. @united This has been the WORST flight experience I've ever had.  [SEP] \"\n",
        "            \"It was neutral sentence. @VirginAmerica Are the hours of operation for the Club at SFO that are posted online current? [SEP] \"\n",
        "            \"It was good sentence. @AmericanAir Thank you for the response, we got it resolved at the counter. [SEP] \"\n",
        "            \"Describe the feeling: It was a sentence [MASK].\"\n",
        "        )\n",
        "\n",
        "        self.verbalizer = {\n",
        "            'good': 2,\n",
        "            'neutral':1,\n",
        "            'bad': 0,\n",
        "        }\n",
        "\n",
        "        self.max_seq_length = 512\n",
        "        self.batch_size = 32\n",
        "\n",
        "\n",
        "config = Config()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "bert_type = 'bert-base-uncased'\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained(bert_type, num_labels = 3)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "bert_config = BertConfig.from_pretrained(bert_type)\n",
        "\n",
        "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
        "\n",
        "#######################################################################\n",
        "#                        End of your code                             #\n",
        "#######################################################################\n",
        "\n",
        "softmax = nn.Softmax(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnmfTTtG7Aa7",
        "outputId": "411cd092-f1ac-4dce-9406-7e9ec3f47432"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXcgMo2TQh6F"
      },
      "source": [
        "## Obtaion verbalizer ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "gJbyfPI7Qh6Q"
      },
      "outputs": [],
      "source": [
        "# Utility function to obtaion verbalizer ids\n",
        "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
        "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
        "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
        "    return verbalizer_ids, index2ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "JYNUrI9YQh6Q"
      },
      "outputs": [],
      "source": [
        "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydex1hr5Qh6Q"
      },
      "source": [
        "## Concatenate original text and prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "gHQK2Ry_yRkS"
      },
      "outputs": [],
      "source": [
        "# Utility function to concatenate prefix and text\n",
        "def concatenate_prefix(texts, config):\n",
        "    ##################################################\n",
        "    #   TODO: concatenate your own prefix and text   #\n",
        "    ##################################################\n",
        "    prefix_texts = [config.prefix + \" \" + \" \".join(text) for text in texts]\n",
        "    ##################################################\n",
        "    #                 End of your code               #\n",
        "    ##################################################\n",
        "    return prefix_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AxuuWgCOQh6Q"
      },
      "outputs": [],
      "source": [
        "def load_data(config):\n",
        "    # ['texts', 'labels']\n",
        "    df = pd.read_csv('./train.csv')\n",
        "    original_texts = df['text'].tolist()\n",
        "    labels = df['sentiment_label'].tolist()\n",
        "\n",
        "    #print(type(original_texts))\n",
        "    #print(type(labels))\n",
        "\n",
        "    texts = concatenate_prefix(original_texts, config)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "texts, labels = load_data(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eZcz8SZAQh6Q"
      },
      "outputs": [],
      "source": [
        "# Batching of texts and labels for training or processing in batches\n",
        "def pack_batch(texts, labels, batch_size):\n",
        "    \"\"\"\n",
        "    :param texts: list\n",
        "    :param labels: list\n",
        "    :param batch_size: int\n",
        "    :return batch_X: list\n",
        "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
        "    :return batch_y: list\n",
        "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
        "    :return batch_count: int\n",
        "    \"\"\"\n",
        "    assert len(texts) == len(labels)\n",
        "\n",
        "    if len(texts) % batch_size != 0:\n",
        "        flag = False\n",
        "        batch_count = int(len(texts) / batch_size) + 1\n",
        "    else:\n",
        "        flag = True\n",
        "        batch_count = int(len(texts) / batch_size)\n",
        "\n",
        "    batch_X, batch_y = [], []\n",
        "\n",
        "    if flag:\n",
        "        for i in range(batch_count):\n",
        "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
        "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
        "    else:\n",
        "        for i in range(batch_count):\n",
        "            if i == batch_count - 1:\n",
        "                batch_X.append(texts[i * batch_size:])\n",
        "                batch_y.append(labels[i * batch_size:])\n",
        "            else:\n",
        "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
        "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
        "\n",
        "    return batch_X, batch_y, batch_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "A1DQHgKOQh6R"
      },
      "outputs": [],
      "source": [
        "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2N7NDFRQh6R"
      },
      "source": [
        "## Inferencing the model without training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JUrPQb4_Qh6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec6c4d3-99ab-45ee-fd0a-ef693bb1ab07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[100 %] Time elapsed: 00:07:20 | ETA: 00:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.626952 | precision: 0.393068 | recall: 0.626952 | f1: 0.483196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Total time elapsed: 00:07:20\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    predict_all = np.array([], dtype=int)\n",
        "    labels_all = np.array([], dtype=int)\n",
        "    pper = pyprind.ProgPercent(batch_count)\n",
        "    for i in range(batch_count):\n",
        "        inputs = batch_X[i]\n",
        "        labels = batch_y[i]\n",
        "\n",
        "        # Using the BERT tokenizer (tokenizer.batch_encode_plus), adding special tokens, ensuring a maximum sequence length, and handling padding/truncation\n",
        "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
        "                                             max_length=config.max_seq_length,\n",
        "                                             padding='max_length', truncation=True)\n",
        "\n",
        "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
        "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
        "\n",
        "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
        "        logits = bert(ids, attention_mask=attention_mask).logits\n",
        "\n",
        "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "        # Find [MASK] logits\n",
        "        # shape: (batch_size, vocab_size)\n",
        "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
        "\n",
        "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
        "        # shape: (batch_size, verbalizer_size)\n",
        "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
        "\n",
        "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
        "        pseudo_distribution = softmax(verbalizer_logits)\n",
        "\n",
        "        #################################################################################\n",
        "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
        "        #   2. Convert the index to the corresponding word ID                           #\n",
        "        #   3. Convert the ID to a token                                                #\n",
        "        #   4. Find the label corresponding to the token                                #\n",
        "        #################################################################################\n",
        "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
        "\n",
        "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
        "\n",
        "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
        "\n",
        "        pred_labels = [config.verbalizer.get(token, -1) for token in pred_tokens]\n",
        "\n",
        "        pred_labels = np.array(pred_labels)\n",
        "\n",
        "        #################################################################################\n",
        "        #                             End of your code                                  #\n",
        "        #################################################################################\n",
        "\n",
        "        predict_all = np.append(predict_all, pred_labels)\n",
        "        labels_all = np.append(labels_all, labels)\n",
        "\n",
        "        pper.update()\n",
        "\n",
        "    acc = accuracy_score(labels_all, predict_all)\n",
        "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
        "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
        "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
        "\n",
        "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **已將每種不同的prompt各自的表現放進report中**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bsOieXbEOrTD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_0zNHhyQh6R"
      },
      "source": [
        "# Task 3: LM-BFF (45 points)\n",
        "\n",
        "https://arxiv.org/pdf/2012.15723.pdf\n",
        "\n",
        "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF-C9mVtQh6R"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3n5c8BPQh6S"
      },
      "source": [
        "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_qGG6UQQh6S"
      },
      "source": [
        "# Install openprompt\n",
        "\n",
        "This library provides a standard, flexible and extensible framework to deploy the prompt-learning pipeline.\n",
        "\n",
        "[OpenPrompt Documentation](https://thunlp.github.io/OpenPrompt/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PZF4PqZuQh6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5cd682-7c93-45d3-df68-9443274b581e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openprompt\n",
            "  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from openprompt) (4.35.2)\n",
            "Collecting sentencepiece==0.1.96 (from openprompt)\n",
            "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.2 in /usr/local/lib/python3.10/dist-packages (from openprompt) (4.66.1)\n",
            "Collecting tensorboardX (from openprompt)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from openprompt) (3.8.1)\n",
            "Collecting yacs (from openprompt)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting dill (from openprompt)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from openprompt)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge==1.0.0 (from openprompt)\n",
            "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from openprompt) (10.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from openprompt) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.4.1)\n",
            "Collecting pyarrow-hotfix (from datasets->openprompt)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (3.4.1)\n",
            "Collecting multiprocess (from datasets->openprompt)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->openprompt) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->openprompt) (1.3.2)\n",
            "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.10.0->openprompt) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->openprompt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->openprompt) (2023.3.post1)\n",
            "Installing collected packages: sentencepiece, yacs, rouge, pyarrow-hotfix, protobuf, dill, tensorboardX, multiprocess, datasets, openprompt\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.99\n",
            "    Uninstalling sentencepiece-0.1.99:\n",
            "      Successfully uninstalled sentencepiece-0.1.99\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.9.2\n",
            "    Uninstalling protobuf-3.9.2:\n",
            "      Successfully uninstalled protobuf-3.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 openprompt-1.0.1 protobuf-4.25.1 pyarrow-hotfix-0.6 rouge-1.0.0 sentencepiece-0.1.96 tensorboardX-2.6.2.2 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install openprompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs7vUWc4Qh6S"
      },
      "source": [
        "# Import openprompt package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3e5I24b8Qh6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23964018-90d4-4919-9dd1-9b7326c7370a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from openprompt.plms import load_plm\n",
        "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
        "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
        "from openprompt.prompts import ManualTemplate\n",
        "from openprompt.trainer import ClassificationRunner\n",
        "import copy\n",
        "import torch\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbXH87eAQh6S"
      },
      "source": [
        "# Setup cuda and whether to perform automatic generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Lf6pHb9gQh6S"
      },
      "outputs": [],
      "source": [
        "cuda = True\n",
        "auto_t = True # Whether to perform automatic template generation\n",
        "auto_v = True # Whether to perform automatic verbalizer generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnjiGD-FQh6S"
      },
      "source": [
        "# Load dataset and model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq ./drive/MyDrive/SST-2.zip"
      ],
      "metadata": {
        "id": "nkXO6TvQXb7S"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "7WAyexkGQh6T"
      },
      "outputs": [],
      "source": [
        "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
        "dataset = {}\n",
        "dataset['train'] = SST2Processor().get_train_examples(\"./SST-2/\")\n",
        "dataset['validation'] = SST2Processor().get_dev_examples(\"./SST-2/\")\n",
        "dataset['test'] = SST2Processor().get_test_examples(\"./SST-2/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpXKTV5tQh6T"
      },
      "outputs": [],
      "source": [
        "#print('load model...')\n",
        "from openprompt.plms import load_plm\n",
        "\n",
        "# load mlm model for main tasks\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
        "\n",
        "# load generation model for template generation\n",
        "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
        "\n",
        "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
        "\n",
        "# if you wish to do automatic label word generation, the verbalizer is not the final verbalizer, and is only used for template generation.\n",
        "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']]) # Manually generate the verbalizer\n",
        "\n",
        "\n",
        "###################################################################################################################\n",
        "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
        "#         compare auto generate template and manual generate template                                             #\n",
        "###################################################################################################################\n",
        "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
        "\n",
        "############################################\n",
        "#   LMBFFTemplateGenerationTemplate        #\n",
        "############################################\n",
        "import random\n",
        "\n",
        "# number of demonstrations\n",
        "num_demonstrations = 1  # try different number\n",
        "\n",
        "demonstrations = []\n",
        "\n",
        "for _ in range(num_demonstrations):\n",
        "    # random choice training set example with label 0\n",
        "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
        "\n",
        "    # random choice training set example with label 1\n",
        "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
        "\n",
        "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
        "    demonstrations.append(demonstration)\n",
        "\n",
        "# You can modify the demonstrations and try different combinations\n",
        "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
        "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "\n",
        "#############################################\n",
        "#   End of LMBFFTemplateGenerationTemplate  #\n",
        "#############################################\n",
        "\n",
        "########################################\n",
        "#          ManualTemplate              #\n",
        "########################################\n",
        "\n",
        "template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
        "\n",
        "########################################\n",
        "#          End of ManualTemplate       #\n",
        "########################################\n",
        "\n",
        "###################################################################################################################\n",
        "#                                           End of your code                                                      #\n",
        "###################################################################################################################\n",
        "\n",
        "\n",
        "# view wrapped example\n",
        "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
        "print(\"dataset:\", dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7iQMKReX--K",
        "outputId": "567f0196-dd27-48d6-e597-e3392df5f904"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaoCe_RuQh6T"
      },
      "source": [
        "# Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqaD-v8hQh6T"
      },
      "outputs": [],
      "source": [
        "from openprompt.plms import load_plm\n",
        "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
        "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
        "from openprompt.prompts import ManualTemplate\n",
        "from openprompt.trainer import ClassificationRunner\n",
        "import copy\n",
        "import torch\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "\n",
        "# Returns the best evaluation score achieved during training\n",
        "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer):\n",
        "    best_score = 0.0\n",
        "    for epoch in range(5):\n",
        "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
        "        score = evaluate(model, val_dataloader)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
        "    return best_score\n",
        "\n",
        "# Trains the model on the training data and computes the training loss\n",
        "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        #####################################################\n",
        "        # 1. Put correct variables into model to get logits #\n",
        "        # 2. Get labels                                     #\n",
        "        # 3. Evalutate using loss_func                         #\n",
        "        # 4. Append loss to loss_all                        #\n",
        "        #####################################################\n",
        "        logits = ...\n",
        "        labels = ...\n",
        "        loss = ...\n",
        "        loss.backward()\n",
        "        loss_all.append(...)\n",
        "        #####################################################\n",
        "        #                 End of your code                  #\n",
        "        #####################################################\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(loss_all)\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "    allpreds = []\n",
        "    alllabels = []\n",
        "    with torch.no_grad():\n",
        "        for step, inputs in enumerate(val_dataloader):\n",
        "            if cuda:\n",
        "                inputs = inputs.cuda()\n",
        "            #####################################################\n",
        "            # 1. Put correct variables into model to get logits #\n",
        "            # 2. Get labels                                     #\n",
        "            # 3. Extend labels to list                          #\n",
        "            # 4. Get predictions and extend preds to list        #\n",
        "            #####################################################\n",
        "            logits = ...\n",
        "            labels = ...\n",
        "            alllabels.extend(...)\n",
        "            allpreds.extend(...)\n",
        "            #####################################################\n",
        "            #                 End of your code                  #\n",
        "            #####################################################\n",
        "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEjjN3sKQh6T"
      },
      "source": [
        "# Automatic template generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3hcb7X0Qh6T"
      },
      "source": [
        "Generated template from TemplateGenerator and find the best template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMdVk0qPQh6U"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class ManualTemplateWithoutParse(ManualTemplate):\n",
        "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
        "    def on_text_set(self):\n",
        "        pass\n",
        "\n",
        "# Template generation\n",
        "if auto_t:\n",
        "    print('performing auto_t...')\n",
        "\n",
        "    if cuda:\n",
        "        template_generate_model = template_generate_model.cuda()\n",
        "\n",
        "    # Creates an instance of T5TemplateGenerator, used for generating text templates\n",
        "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
        "\n",
        "\n",
        "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
        "    for data in dataloader:\n",
        "        if cuda:\n",
        "            data = data.cuda()\n",
        "        template_generator._register_buffer(data)\n",
        "\n",
        "    template_generate_model.eval()\n",
        "    print('generating...')\n",
        "    template_texts = template_generator._get_templates() # Calls _get_templates on template_generator to generate template texts.\n",
        "\n",
        "    # Converting and Printing Templates\n",
        "    original_template = template.text\n",
        "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
        "    # template_generator._show_template()\n",
        "    template_generator.release_memory()\n",
        "    # Generate a number of candidate template text\n",
        "    print(template_texts)\n",
        "\n",
        "    # Iterate over each candidate and select the best one\n",
        "    best_metrics = 0.0\n",
        "    best_template_text = None\n",
        "    for template_text in tqdm(template_texts):\n",
        "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']])\n",
        "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "        print(f\"current template: {template_text}, wrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
        "\n",
        "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
        "\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "        if cuda:\n",
        "            model = model.cuda()\n",
        "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
        "\n",
        "        #######################################################\n",
        "        # TODO: Use score to Find your best template_text     #\n",
        "        #######################################################\n",
        "        ...\n",
        "        #######################################################\n",
        "        #                 End of your code                    #\n",
        "        #######################################################\n",
        "    # Use the best template\n",
        "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']])\n",
        "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "    print(\"final best template:\", best_template_text)\n",
        "    print(\"wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSGvSGARQh6U"
      },
      "source": [
        "# Automatic erbalizer generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l955T61GQh6U"
      },
      "source": [
        "Verbalizer template from VerbalizerGenerator and find the best verbalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVqR8e4UQh6U"
      },
      "outputs": [],
      "source": [
        "# Verbalizer generation\n",
        "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
        "if auto_v:\n",
        "    print('performing auto_v...')\n",
        "    # Load generation model for verbalizer generation\n",
        "    if cuda:\n",
        "        plm = plm.cuda()\n",
        "\n",
        "    # Creates an instance of RobertaVerbalizerGenerator, used for generating verbalizer.\n",
        "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20) # To improve performance, try larger numbers\n",
        "\n",
        "\n",
        "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
        "    for data in dataloader:\n",
        "        if cuda:\n",
        "            data = data.cuda()\n",
        "        verbalizer_generator.register_buffer(data)\n",
        "\n",
        "    # Calls generate on verbalizer_generator to generate label words.\n",
        "    label_words_list = verbalizer_generator.generate()\n",
        "    verbalizer_generator.release_memory()\n",
        "\n",
        "    # Iterate over each candidate and select the best one\n",
        "    current_verbalizer = copy.deepcopy(verbalizer)\n",
        "    best_metrics = 0.0\n",
        "    best_label_words = None\n",
        "    for label_words in tqdm(label_words_list):\n",
        "        current_verbalizer.label_words = label_words\n",
        "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
        "\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "        if cuda:\n",
        "            model = model.cuda()\n",
        "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
        "\n",
        "        #######################################################\n",
        "        # TODO: Use score to find your best_label_word        #\n",
        "        #######################################################\n",
        "        ...\n",
        "        #######################################################\n",
        "        #                 End of your code                    #\n",
        "        #######################################################\n",
        "    # use the best verbalizer\n",
        "    print(\"final best label words:\", best_label_words)\n",
        "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZlhAbUNQh6U"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDk9XixbQh6V"
      },
      "outputs": [],
      "source": [
        "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "\n",
        "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq1IVWLoQh6V"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLBDX2qWQh6V"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "allpreds = []\n",
        "for step, inputs in enumerate(test_dataloader):\n",
        "    if cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = model(inputs)\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "with open('pred.csv', 'w') as f:\n",
        "    f.write('index,sentiment_label\\n')\n",
        "    for i, pred in enumerate(allpreds):\n",
        "        f.write('{},{}\\n'.format(i, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mghkS5jDQh6V"
      },
      "source": [
        "# Report (15 points)\n",
        "\n",
        "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
        "\n",
        "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
        "\n",
        "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "51ee1b965d6f75a20b2b6babb72920dce4fab5775c12eb1659af0fb55d185fed"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}